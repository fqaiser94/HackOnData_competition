{"cells":[{"cell_type":"markdown","source":["# Hybrid, Scalable, Online Recommendations\n\n_Diego Fanesi, Farooq Qaiser, Sai Chaitanya_"],"metadata":{}},{"cell_type":"markdown","source":["## Introduction \n\nIf you've ever shopped on Amazon, or listened to music on Spotify, or watched videos on Youtube, you've probably used their recommendation system, perhaps without even realizing it. A reccommendation system will typically point out one or two other pieces content that you might like, such as the ones below."],"metadata":{}},{"cell_type":"code","source":["links = ['http://blog.sendblaster.com/wp-content/uploads/amazon-email-reccomend.jpg',\n         'https://cdn.vox-cdn.com/thumbor/A-wdLgp-Wm0cQ3aM_lXvgfdDRqc=/cdn.vox-cdn.com/uploads/chorus_asset/file/4109214/Discover_Weekly_Snapshot.0.png',\n         'https://cdn-images-1.medium.com/max/1248/0*xLPGdyqD3SoF-38C.', \n         'http://spc.columbiaspectator.com/sites/default/files/netflix_0.jpg'\n        ]\n\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in links]\n\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Recommendation systems have quickly become a ubiqitious part of the web experience. Almost everything you read, see, or buy on the internet these days has been selected by an algorithm. That includes news articles on Google, status updates on Facebook, products shown on Amazon, or movies on Netflix.\n\nFrom a user standpoint, reccommendation systems allow users to quickly and easily find related/useful content, tailored specifically to their preferences. According to a [McKinsey report](http://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers) 35% of what consumers purchase on Amazon and 75% of what they watch on Netflix comes from product recommendations.  \n\nThe value of reccommendation systems for businesses is also well established in literature. For example, [Lee, D & Hosanagar, K. (2014)](https://www.researchgate.net/publication/289062986_Impact_of_recommender_systems_on_sales_volume_and_diversity) show that purchase-based collaborative filtering methods (\"Consumers who bought this item also bought\") caused a 25% lift in views and a 35% lift in the number of items purchased over the control group (no recommender) for those who purchase. Similarly, according to a [paper](http://dl.acm.org/citation.cfm?id=2843948) published by executives at Netflix (the popular on-demand video streaming service), they estimate that the company is able to save $1 billion per year by reccommending new content to viewers so they keep watching and don’t cancel their accounts.      \n\nGiven their incredible value and commercial importance, we chose to build a reccommendation system for our HackOn(Data) 2017 project.   \n\n### Problem statement\n\n\n![Amazon_home_page_mock](https://lh5.googleusercontent.com/J8vlDc1A9VuYSllInFrmhk05gAj1IghcrzC8RyxZpIObqR5kb-yEyE1QwKqTuzlWODQh03Cn=w1920-h971 \"image description\")\n\nLet’s say we have a user called Molly and she’s currently browsing on Amazon. She and her husband have just had a baby and she’s looking for books to help her understand baby sign language. What she wants to be able to do is easily find other books about baby sign language. In addition, our data shows that she’s highly likely to be interested in romance novels as well so we reccommend some some romance novels she might also like.   \n\nHowever, we don’t want to do this just for Molly! We want to do it for all of our other millions of users. In other words, we want to do this at scale!  \n\nIn addition, we also want to do this in real time. If Molly is looking at a book about baby sign language right now, we want to be able to reccommend baby products to her also right now. \n\nThis is exactly what we were trying to do for our project. Below, we summarise the nature of the problems we tackled, our expermiments and the final solution.  \n\n![Problem_experiment_solution](https://lh4.googleusercontent.com/hSJao0xNdQMUmTWSMEvJgHCz6UkqG4QrzseGKcZ_uKxozzZrEedbILZWkmspLLtb9ohY2DLqOaaq2q8=w1366-h659 \"image description\")\n\n### How we’re solving the problem\n\nWe build a reccommendation engine that leverages:   \n  \n1. A hybrid (content based and collaborative filtering) approach to produce robust reccommendations     \n  \n2. The Spark framework to produce a scalable implementation  \n  \n3. Advanced techniques such as Locality Sensitive Hashing (LSH) and Stochastic Gradient Descent (SGD) to demonstrate real-time capabilities  \n\n### How this notebook is laid out\n\n1. Data preparation\n2. Exploratory data analysis\n3. Baseline recommender\n4. Content based filtering (N^2) recommender\n5. Content based filtering (Locality Sensitive Hashing) recommender\n6. Collaborative filtering (Alternating Least Squares) recommender\n7. Collaborative filtering (Schotastic Gradient Descent) recommender\n8. Conclusions\n9. Future areas of improvement and research areas \n\nNext we introduce basic principles behind each type of recommendation system. \n\n### A brief introduction to how reccommendation systems work  \n\nReccommendation engines can be broadly categorized as one of the following:  \n  \n1. Content based methods  \n   These make a recommendation on the basis of a user profile constructed from features of content they've previously viewed.    \n     \n2. Collaborative filtering methods  \n   These make a recommendation by finding users with similar tastes to other users and recommending items that those other users also liked.  \n     \n3. Hybrid  methods  \n   These are usually a combination of the previous two methods.\n\nThe majority of reccommendation engines in production use today are of the hybrid variety. Netflix, is again a good example of this. They make recommendations by comparing the viewing and searching habits of similar users (i.e. collaborative filtering) as well as by offering content that shares characteristics with content that a user has previously rated highly (i.e. content based filtering).  \n\nThat's it for now, we explain each of these approaches in  detail in their respective sections below.  \n  \n### Credits\n\nWe would like to thank Julian McAuley for sharing the Amazon datasets that were used for this project.  \n_R. He, J. McAuley. Modeling the visual evolution of fashion trends with one-class collaborative filtering. WWW, 2016_   \n_J. McAuley, C. Targett, J. Shi, A. van den Hengel. Image-based recommendations on styles and substitutes. SIGIR, 2015_"],"metadata":{}},{"cell_type":"markdown","source":["## Administrative stuff"],"metadata":{}},{"cell_type":"markdown","source":["Notebook parameters"],"metadata":{}},{"cell_type":"code","source":["# just the variable...\nMOUNT_NAME = \"s3storage\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["seed = 1800009193L"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# data manipulation \nfrom pyspark.sql.functions import col,monotonically_increasing_id, split, regexp_extract, regexp_replace, split, explode, lit, when, count, avg, concat, round, array, udf, rank, from_unixtime, length, ltrim\n\nfrom pyspark.sql.types import IntegerType, DoubleType, BooleanType, StringType\nfrom pyspark.sql.window import Window\nfrom pyspark import StorageLevel\n\nimport pandas as pd\nimport numpy as np\nimport re\n\n# machine learning \nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Load data"],"metadata":{}},{"cell_type":"code","source":["#Farooq Dataset\nrawMetasDF = spark.read.parquet(\"/mnt/%s/meta_Books.parq\" % MOUNT_NAME).cache() # full dataset\nprint rawMetasDF.count()\n\n#Diego Dataset\nrawRatingsDF = spark.read.parquet(\"/mnt/%s/parquetDataset/ratings_Books.parquet\" % MOUNT_NAME).cache()\nprint rawRatingsDF.count()\n\n#Sai Dataset\nrawReviewsDF = sqlContext.read.parquet(\"/mnt/%s/parquetDataset/reviews_Books_5_2.parq\" % MOUNT_NAME).cache()\nprint rawReviewsDF.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Let's take a quick look at our datasets."],"metadata":{}},{"cell_type":"code","source":["print('rawRatingsDF')\nrawRatingsDF.show(5)\n\nprint('rawMetasDF')\nrawMetasDF.show(5)\n\nprint('rawReviewsDF')\nrawReviewsDF.show(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["There's a few things to note here.  \nFirstly, most of our data is focused on the products, rather than the users.  \nSecondly, we have missing data.  \nThirdly, there is some bad data (malformed rows), see below for a specific example at how reviewTime doesn't actually contain a time."],"metadata":{}},{"cell_type":"code","source":["rawReviewsDF.filter('reviewerID == \"A1MCAHDE1F3Q6L\" AND asin = \"000100039X\"').show(1)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Some of this data is going to be a little hard to work with.  \nWe'll take care of that in the Data Preparation section up next."],"metadata":{}},{"cell_type":"markdown","source":["## Data preparation\n\nIn this section, we perform some basic data manipulation techniques to make it easier to extract data for modelling later on."],"metadata":{}},{"cell_type":"markdown","source":["### Useable data format  \nFirst we transform our data into a useable shape.  \nThe rawRatingsDF looks like its in good shape. \n\nOn the other hand, the rawMetasDF doesn't look so great. It would be more useful as severak dataframes as some columns are essentially denormalized structures. Let's fix that now."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import split, regexp_extract, regexp_replace, split, explode, lit, when\n\n\n# extract data in a friendly format\ntemp = (rawMetasDF\n        #.withColumn('general', regexp_extract('related', '\\\\[(.*?)\\\\]', 1))\n        .withColumn('bought_together', regexp_extract('related', '\"bought_together\": \\\\[(.*?)\\\\]', 1))\n        .withColumn('also_bought', regexp_extract('related', '\"also_bought\": \\\\[(.*?)\\\\]', 1))\n        .withColumn('buy_after_viewing', regexp_extract('related', '\"buy_after_viewing\": \\\\[(.*?)\\\\]', 1))\n        .withColumn('also_viewed', regexp_extract('related', '\"also_viewed\": \\\\[(.*?)\\\\]', 1))\n        .select('asin'\n                , 'salesRank'\n                , 'imUrl'\n                , 'categories'\n                , 'title'\n                ,'description'\n                , 'price'\n                , 'brand'\n                , 'bought_together'\n                , 'also_bought'\n                , 'buy_after_viewing'\n                , 'also_viewed'\n               )\n       )\n\n#print(\"temp\")\n#temp.show(5)\n\n\n\n\n# extract product attributes data and clean up\nattributesDF = (temp\n                .select('asin', 'title', 'brand', 'description', 'price', 'imUrl')\n                .distinct()\n               )\n\nprint(\"attributesDF\")\nattributesDF.show(5)\n\n\n\n\n# extract categories data and clean up\n# note each product can be associated with multiple categories\ncategoriesDF = (temp\n                .select('asin', 'categories')\n                .withColumn('categories', regexp_extract('categories', '\\\\[\\\\[(.*?)\\\\]', 1))\n                .withColumn('categories', regexp_replace(\"categories\", '\\\\\"', \"\"))\n                .filter(\"categories != ''\")\n                .select('asin', explode(split(col(\"categories\"), \", \")).alias(\"categories\"))\n                .distinct()\n                )\n\nprint(\"categoriesDF\")\ncategoriesDF.show(5)\n\n\n\n\n# extract salesRank data and clean up\nsalesRankDF = (temp\n               .select('asin', col('salesRank').alias('salesRankMessy'))\n               # extract everything between curly braces\n               .withColumn('salesRankMessy', regexp_extract('salesRankMessy', '\\\\{(.*?)\\\\}', 1))\n               # remove quotes\n               .withColumn('salesRankMessy', regexp_replace(\"salesRankMessy\", '\\\\\"', \"\"))\n               .filter(\"salesRankMessy != ''\")\n               # normalize structure\n               .select('asin', explode(split(col(\"salesRankMessy\"), \", \")).alias(\"salesRankMessy\"))\n               # extract salesRankCategory\n               .withColumn('salesRankCategory', regexp_extract('salesRankMessy', '(\\w+)', 1))\n               # extract salesRank\n               .withColumn('salesRank', regexp_extract('salesRankMessy', '(\\d+)', 1))\n               .select('asin', 'salesRankCategory', 'salesRank')\n               .distinct()\n              )\n\nprint(\"salesRank\")\nsalesRankDF.show(5)\n\n\n\n\n# explode each category\nbought_together = (temp\n                   .select('asin', explode(split(col(\"bought_together\"), \", \")).alias(\"related_asin\"))\n                   .withColumn(\"related\", lit(\"bought_together\"))\n                  )\n\nalso_bought = (temp\n               .select('asin', explode(split(col(\"also_bought\"), \", \")).alias(\"related_asin\"))\n               .withColumn(\"related\", lit(\"also_bought\"))\n              )\n\nbuy_after_viewing = (temp\n                     .select('asin', explode(split(col(\"buy_after_viewing\"), \", \")).alias(\"related_asin\"))\n                     .withColumn(\"related\", lit(\"buy_after_viewing\"))\n                    )\n\nalso_viewed = (temp\n               .select('asin', explode(split(col(\"also_viewed\"), \", \")).alias(\"related_asin\"))\n               .withColumn(\"related\", lit(\"also_viewed\"))\n              )\n\n# bring all categories together\nrelatedProductsDF = (bought_together\n                     .unionAll(also_bought)\n                     .unionAll(buy_after_viewing)\n                     .unionAll(also_viewed)\n                     .filter(\"related_asin != ''\")\n                     .withColumn('related_asin', regexp_replace(\"related_asin\", '\\\\\"', \"\"))\n                     .select('asin', 'related', 'related_asin')\n                    )\n\nprint(\"relatedProductsDF\")\nrelatedProductsDF.show(5)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Convert userId and productId to integers\nOne of the requirements of some of the algorithms that we apply (e.g. ALS) is that userId and productId be in integer format.  \nThe datasets provided by James McAuley do not fulfil these requirements as the userId and productId are in alphanumeric format.  \nAs a result, our first stage of data preparation is to convert all userId and productId to an integer format across all of our datasets."],"metadata":{}},{"cell_type":"markdown","source":["First we extract all the userId and productId we can find across all of our datasets"],"metadata":{}},{"cell_type":"code","source":["# rawMetasDF contains only products\nrawMetasDF_products = (rawMetasDF\n                       .select(col(\"asin\"))\n                       .withColumn('raw_MetasDF_asin', lit(1))\n                       .distinct()\n                      )\n\n# print('rawMetasDF_products')\n# rawMetasDF_products.show(5)\n\n\n\nrawMetasDF_related_products = (rawMetasDF\n                               .select(col('related'))\n                               .filter('related is not null')\n                               .withColumn('general', regexp_extract('related', '\\\\[(.*?)\\\\]', 1))\n                               .select(explode(split(col(\"general\"), \", \")).alias(\"asin\"))\n                               .filter(\"asin != ''\")\n                               .withColumn('asin', regexp_replace(\"asin\", '\\\\\"', \"\"))\n                               .withColumn('raw_MetasDF_related', lit(1))\n                               .distinct()\n                               )\n\n# print('rawMetasDF_related_products')\n# rawMetasDF_related_products.show(5)\n\n\n\n\n# rawRatingsDF contains both users and products\nrawRatingsDF_users = (rawRatingsDF\n                      .select(col(\"reviewerID\"))\n                      .withColumn('rawRatingsDF_reviewerID', lit(1))\n                      .distinct()\n                      )\n\n# print('rawRatingsDF_users')\n# rawRatingsDF_users.show(5)\n\n\n\nrawRatingsDF_products = (rawRatingsDF\n                         .select(col(\"asin\"))\n                         .withColumn('rawRatingsDF_asin', lit(1))\n                         .distinct()\n                        )\n\n# print('rawRatingsDF_products')\n# rawRatingsDF_products.show(5)\n\n\n\n\n# rawReviewsDF contains both users and products\nrawReviewsDF_users = (rawReviewsDF\n                      .select(col(\"reviewerID\"))\n                      .withColumn('rawReviewsDF_reviewerID', lit(1))\n                      .distinct()\n                      )\n\n# print('rawReviewsDF_users')\n# rawReviewsDF_users.show(5)\n\n\n\nrawReviewsDF_products = (rawReviewsDF\n                         .select(col(\"asin\"))\n                         .withColumn('rawReviewsDF_asin', lit(1))\n                         .distinct()\n                        )\n\n# print('rawReviewsDF_products')\n# rawReviewsDF_products.show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["First let's build those master lists of all userId and productId across all of our datasets.  \nThen we can check if there are any userId or productId that don't appear across all datasets."],"metadata":{}},{"cell_type":"code","source":["# users\nall_users = (rawRatingsDF_users\n             .join(rawReviewsDF_users, 'reviewerID', 'fullouter')\n            )\n\n# print('users not present in all datasets')\n# (all_users\n#  .where(col(\"rawRatingsDF_reviewerID\").isNull() \n#         | col(\"rawReviewsDF_reviewerID\").isNull() \n#        )\n#  .show(5)\n#  )\n\n\n\n# products\nall_products = (rawMetasDF_products\n                .join(rawMetasDF_related_products, 'asin', 'fullouter')\n                .join(rawRatingsDF_products, 'asin', 'fullouter')\n                .join(rawReviewsDF_products, 'asin', 'fullouter')\n               )\n\n# print('products not present in all datasets')\n# (all_products\n#  .where(col(\"raw_MetasDF_asin\").isNull() \n#         | col(\"raw_MetasDF_related\").isNull() \n#         | col(\"rawRatingsDF_asin\").isNull()\n#         | col(\"rawReviewsDF_asin\").isNull()\n#        )\n#  .show(5))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Alright, let's use those master lists we built above and assign unique (integer-based) IDs to each user and product."],"metadata":{}},{"cell_type":"code","source":["userConversionTableDF = (all_users\n                         .coalesce(1)\n                         .select(col(\"reviewerID\"))\n                         .distinct()\n                         .withColumn(\"newUserId\", monotonically_increasing_id())\n#                          .cache()\n                         )\n\n# print('userConversionTableDF')\n# userConversionTableDF.show(5)\n\nproductConversionTableDF = (all_products\n                            .coalesce(1)\n                            .select(col(\"asin\"))\n                            .distinct()\n                            .withColumn(\"newProductId\", monotonically_increasing_id())\n#                             .cache()\n                            )\n\n# print('productConversionTableDF')\n# productConversionTableDF.show(5)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Finally we convert the Id in all datasets to our new ID using the mapping tables we created above."],"metadata":{}},{"cell_type":"code","source":["ratingsDF_newID = (rawRatingsDF\n                   .join(userConversionTableDF, \"reviewerID\", \"left\")\n                   .join(productConversionTableDF, \"asin\", \"left\")\n                   .select(col('newProductId').cast(IntegerType()).alias(\"productId\")\n                           , col('newUserId').cast(IntegerType()).alias(\"userId\")\n                           , col(\"rating\") \n                           , from_unixtime(col(\"Timestamp\")).cast('date').alias(\"Timestamp\")\n                          )\n                   .cache()\n                  )\n\nprint('ratingsDF_newID')\nratingsDF_newID.show(10)\n\n\n\n\nattributesDF_newID = (attributesDF\n                      .join(productConversionTableDF, \"asin\", \"left\")\n                      .select(col('newProductId').cast(IntegerType()).alias(\"productId\"),\n                              'title', \n                              'brand', \n                              'description', \n                              'price', \n                              'imUrl')\n                      .cache()\n                     )\n\nprint(\"attributesDF_newID\")\nattributesDF_newID.show(5)\n\n\n\n\ncategoriesDF_newID = (categoriesDF\n                      .join(productConversionTableDF, \"asin\", \"left\")\n                      .select(col('newProductId').cast(IntegerType()).alias(\"productId\"),\n                              \"categories\")\n                     )\n\nprint(\"categoriesDF_newID\")\ncategoriesDF_newID.show(5)\n\n\n\n\nsalesRankDF_newID = (salesRankDF\n                     .join(productConversionTableDF, \"asin\", \"left\")\n                     .select(col('newProductId').cast(IntegerType()).alias(\"productId\"),\n                             'salesRankCategory', \n                             'salesRank')\n                     .cache()\n                    )\n\nprint(\"salesRankDF_newID\")\nsalesRankDF_newID.show(5)\n\n\n\n\nrelatedProductsDF_newID = (relatedProductsDF\n                           .join(productConversionTableDF, \n                                 \"asin\", \n                                 \"left\")\n                           .join(productConversionTableDF.select('asin', col('newProductId').alias('new_related_asin_Id')), \n                                 relatedProductsDF.related_asin == productConversionTableDF.asin, \n                                 \"left\")\n                           .select(col('newProductId').cast(IntegerType()).alias(\"productId\"),\n                                   'related',\n                                   col('new_related_asin_Id').cast(IntegerType()).alias(\"related_productId\")\n                                  )\n                           .cache()\n                          )\n\nprint(\"relatedProductsDF_newID\")\nrelatedProductsDF_newID.show(5)\n\n\n\n\n# ReviewsDF\nreviewsDF_newID = (rawReviewsDF\n                   .join(userConversionTableDF, \"reviewerID\", \"left\")\n                   .join(productConversionTableDF, \"asin\", \"left\")\n                   .select(col('newUserId').cast(IntegerType()).alias(\"userId\"),\n                           col('newProductId').cast(IntegerType()).alias(\"productId\"),\n                           col(\"overall\"), \n                           col(\"reviewerName\"), \n                           col(\"helpful\"), \n                           col(\"reviewText\"), \n                           col(\"summary\"), \n                           from_unixtime(col(\"unixReviewTime\")).cast('date').alias(\"Timestamp\"),  \n                           col(\"reviewTime\")\n                          )\n                   .cache()\n                  )\n\nprint('reviewsDF_newID')\nreviewsDF_newID.show(5)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["There is ratings data in both rawRatingsDF and rawReviewsDF. Let's create a unified dataset of training data from these 2 datasets."],"metadata":{}},{"cell_type":"code","source":["#unify the two datasets. \nratingsDF_complete = (reviewsDF_newID\n                      .select(\"productId\"\n                              , \"userId\"\n                              , col(\"overall\").alias(\"rating\").cast(IntegerType())\n                              , \"Timestamp\"\n                             )\n                      .unionAll(ratingsDF_newID\n                                .select(\"productId\"\n                                        , \"userId\"\n                                        , col(\"rating\").cast(IntegerType())\n                                        , \"Timestamp\"\n                                       )\n                               )\n                      .distinct()\n                      .filter(\"rating>=1 and rating<=5\")\n                      .cache()\n                     )\n\nprint('ratingsDF_complete')\nratingsDF_complete.show(5)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Un-cache objects that are no longer necessary to save memory."],"metadata":{}},{"cell_type":"code","source":["rawMetasDF.unpersist()\nrawRatingsDF.unpersist()\nrawReviewsDF.unpersist()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## Exploratory Data Analysis"],"metadata":{}},{"cell_type":"code","source":["users_val = userConversionTableDF.count()\nitems_val = productConversionTableDF.count()\nratings_val = ratingsDF_complete.count()\nreviews_val = reviewsDF_newID.count()\n\n\nprint('#users: %s' %users_val)\nprint('#items: %s' %items_val)\nprint('#ratings: %s' %ratings_val)\nprint('#reviews: %s' %reviews_val)\n\n# userConversionTableDF.unpersist()\n# productConversionTableDF.unpersist()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["This is big data."],"metadata":{}},{"cell_type":"code","source":["# create temporary views for R to read from for visualizations\ncategoriesDF_newID.createOrReplaceTempView(\"categoriesDF_newID\")\nratingsDF_complete.createOrReplaceTempView(\"ratingsDF_complete\")\nreviewsDF_newID.createOrReplaceTempView(\"reviewsDF_newID\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%r\n\n# load some packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(reshape2)\nlibrary(SparkR)\n\n\n# color palette\ncolor1 = '#0145ac' # dark blue\ncolor1 = 'white' # white\ncolor2 = '#82c7a5' # light blue\ncolor3 = '#eece1a' # yellow\n\ntext_color1 = 'white'\n\nbg_color1 = '#1b212c'\n\n# text size\n\ntext_size1 = 12\n\naxis_title_size1 = 16\n\ntitle_size1 = 20\n\n# theme for charts\ntheme.chart <- \n  theme_classic() + \n  theme(legend.position = \"none\", \n        axis.ticks = element_blank(), \n        axis.line.x = element_line(color = 'white'), \n        axis.line.y = element_line(color = 'white'), \n        axis.title = element_text(colour = text_color1, size = axis_title_size1),\n        panel.background = element_rect(fill = bg_color1),\n        plot.background = element_rect(fill = bg_color1), \n        plot.title = element_text(colour = text_color1, size = title_size1)\n       )"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\nSELECT \n  categories AS Category\n  , COUNT(*) AS Count\nFROM \n  categoriesDF_newID \nGROUP BY \n  categories\nHAVING\n  COUNT(*)>5000\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf %>%\n  dplyr::filter(Category!='Books')\n\ntemp$Category <- factor(temp$Category, \n                       levels = rev(temp$Category[order(temp$Count)]))\n\ng <- ggplot(temp, aes(Category, Count)) + \n  geom_bar(stat=\"identity\", width = 0.5, fill = color1) + \n  theme.chart +\n  labs(title=\"Distribution of Products by Category\") +\n  theme(axis.text.x = element_text(angle = 90, vjust=0.6, color = text_color1),\n        axis.text.y = element_text(color = text_color1)\n       ) + \n  scale_y_continuous(labels = comma)\n\ng"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["A few things to note about this visualization, a book can in several categories.  \nSecondly, there is a long tail of book categories that is not fully visualized here.  \nLastly, most books aren't identified by any category."],"metadata":{}},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\nSELECT \n  rating\n  , COUNT(*) AS Count\nFROM \n  ratingsDF_complete \nGROUP BY \n  rating\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf\n\ntemp$ratings <- factor(temp$rating, \n                       levels = rev(temp$rating[order(temp$rating)]))\n\ng <- ggplot(temp, aes(rating, Count)) + \n  geom_bar(stat=\"identity\", width = 0.5, fill = color1) + \n  labs(title=\"Distribution of Ratings\") +\n  theme.chart +\n  theme(axis.text.x = element_text(size = 12, vjust=0.5, color = text_color1),\n        axis.text.y = element_text(size = 12, color = text_color1)\n       ) + \n  scale_y_continuous(labels = comma)\ng"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["We see that ratings are skewed towards higher ratings.   \nThis isn't very surprising as this has been well noted in academic research."],"metadata":{}},{"cell_type":"code","source":["%r\n\n# collect data\nsparkdf <- sql(\"\nSELECT \n  categories\n  , rating\n  , COUNT(*) AS count\nFROM \n  ratingsDF_complete AS a\n  LEFT JOIN categoriesDF_newID AS b\n    ON a.productId = b.productId\nWHERE\n  categories!='Books'\nGROUP BY \n  categories\n  , rating\nHAVING \n  COUNT(*) > 20000\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf\n\ntemp$rating <- factor(temp$rating, \n                      levels = rev(temp$rating[order(temp$rating)]))\n\n# plot\ng <- ggplot(temp, aes(x = rating, y = count)) + \n  geom_bar(stat=\"identity\", fill = color1) + \n  scale_x_discrete(drop = TRUE) +\n  theme.chart +\n  labs(title=\"Distribution of Ratings\") +\n  facet_wrap(~categories, scales = \"free\") + \n  theme(strip.text.x = element_text(size = 6, colour = \"white\"), \n        strip.background = element_blank(), \n        axis.text.x = element_text(vjust=0.5, color = text_color1),\n        axis.text.y = element_text(color = text_color1)\n       ) \n\ng"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["We see that this bias applies in general to all categories."],"metadata":{}},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\n  SELECT\n    YEAR(Timestamp) AS year\n    , MONTH(Timestamp) AS month\n    , AVG(Rating) AS rating\n  FROM \n    ratingsDF_complete\n  GROUP BY \n    YEAR(Timestamp)\n    , MONTH(Timestamp)\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf %>%\n  dplyr::mutate(date = lubridate::dmy(paste('01', month, year, sep = '/')))\n  \ng <- ggplot(temp, aes(x = date, y = rating)) + \n  geom_line(color = color1) +  \n  geom_hline(yintercept = mean(temp$rating), linetype=\"dotted\", color = color2) +\n  theme.chart +\n  labs(title=\"Average Product Rating Over Time\", \n       x = 'Time', \n       y = 'Rating') +\n  theme(axis.text.x = element_text(angle = 90, vjust=0.5, color = text_color1),\n        axis.text.y = element_text(vjust=0.5, color = text_color1)\n       ) +\n  scale_y_continuous(limits = c(1,5)) +\n  scale_x_date(date_breaks = \"6 month\", date_labels =  \"%b %Y\")\n\ng"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["The average product rating has remained consistently high throughout the period this dataset is concerned with.  \nThe earliest part of the dataset shows a great deal of variance due to fewer observations."],"metadata":{}},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\n  SELECT\n    YEAR(Timestamp) AS year\n    , MONTH(Timestamp) AS month\n    , COUNT(*) AS reviews\n  FROM \n    reviewsDF_newID\n  GROUP BY \n    YEAR(Timestamp)\n    , MONTH(Timestamp)\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf %>%\n  dplyr::mutate(date = lubridate::dmy(paste('01', month, year, sep = '/')))\n  \ng <- ggplot(temp, aes(x = date, y = reviews)) + \n  geom_line(color = color1) +  \n  theme.chart +\n  labs(title=\"Number of Reviews over Time\") +\n  theme(axis.text.x = element_text(angle=90, vjust=0.6, color = text_color1),\n        axis.text.y = element_text(color = text_color1)\n       ) + \n  scale_y_continuous(labels = comma) +\n  scale_x_date(date_breaks=\"1 year\", date_labels=\"%Y\")\n\ng"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["The number of reviews has grown steadily over the years."],"metadata":{}},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\n\nWITH \n\ndaily_count AS (\n  SELECT\n    YEAR(Timestamp) AS year\n    , MONTH(Timestamp) AS month\n    , DAY(Timestamp) AS day\n    , COUNT(*) AS ratings\n  FROM \n    ratingsDF_complete\n  WHERE\n    YEAR(Timestamp) >= 2013\n  GROUP BY \n    YEAR(Timestamp)\n    , MONTH(Timestamp)\n    , DAY(Timestamp)\n)\n\n  SELECT\n    day\n    , AVG(ratings) AS ratings\n  FROM \n    daily_count\n  GROUP BY \n    day\n\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf %>%\n  dplyr::mutate(date = lubridate::dmy(paste(day, '01', '2014', sep = '/')))\n  \ng <- ggplot(temp, aes(x = date, y = ratings)) + \n  geom_line(color = 'white') +\n  geom_hline(yintercept = mean(temp$ratings), linetype=\"dotted\", color = color2) +\n  theme.chart +\n  labs(title=\"Average Number of Daily Ratings Since January 2013\"\n       #,  x = \"Day of Month\",\n       #, y = \"Number of Products Rated\"\n      ) +\n  theme(axis.text.x = element_text(vjust=0.6, color = text_color1, size = text_size1),\n        axis.text.y = element_text(color = text_color1, size = text_size1),\n        axis.title = element_blank()\n       ) +\n  scale_y_continuous(labels = comma, expand = c(0, 0), limits = c(0, 25000)) +\n  scale_x_date(date_breaks=\"5 day\", date_labels=\"%d\")\n\ng"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["2014 - January - number of reviews/ratings coming in daily.  \nmax daily rate and average daily rate, from whole dataset."],"metadata":{}},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\nSELECT\n  userId\n  , productId\n  , rating\nFROM \n  ratingsDF_complete\nWHERE\n userId IN (SELECT DISTINCT userId FROM ratingsDF_complete LIMIT 300)\n\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf\n\ntemp$rating <- factor(temp$rating, \n                      levels = rev(temp$rating[order(temp$rating)]))\n\ntemp$userId <- factor(temp$userId, \n                      levels = rev(temp$userId[order(temp$userId)]))\n\ntemp$productId <- factor(temp$productId, \n                         levels = rev(temp$productId[order(temp$productId)]))\n\ng <- ggplot(temp, aes(x = userId, y = productId)) + \n  geom_tile(fill = 'white') +\n  theme.chart +\n  labs(title=\"User - Product Matrix\", \n       x = 'User',\n       y = 'Product') +\n  theme(axis.text = element_blank()\n       )\n  \ng"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%r\n\ntotal_number_of_ratings = nrow(temp)\n\ntotal_number_of_products = nrow(temp %>% dplyr::select(productId) %>% dplyr::distinct())\n\ntotal_number_of_users = nrow(temp %>% dplyr::select(userId) %>% dplyr::distinct())\n\ntotal_number_of_potential_ratings = total_number_of_products * total_number_of_users\n\nprint(paste0('total number of ratings: ', total_number_of_ratings))\nprint(paste0('total number of products: ', total_number_of_products))\nprint(paste0('total number of users: ', total_number_of_users))\nprint(paste0('total number of potential ratings: ', total_number_of_potential_ratings))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["As expected, the ratings dataset is quite sparse."],"metadata":{}},{"cell_type":"code","source":["# We could leverage the `histogram` function from the RDD api\nhistogram = (ratingsDF_complete\n             .groupBy('userId')\n             .count()\n             .sample(False, 0.01, seed = seed)\n            )\n\n# print(histogram.count()) #80,128 records\n\nhistogram.createOrReplaceTempView(\"histogram\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"SELECT * FROM histogram\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf "],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%r \n\ng <- ggplot(data = temp, aes(x = count)) + \n  geom_histogram(fill = color1) +\n  theme.chart +\n  labs(title=\"Histogram of Number of Ratings by Users\", \n       x = 'Number of Ratings by User',\n       y = 'Frequency (log 10)') +\n  theme(axis.text.x  = element_text(angle = 90, vjust=0.5, color = text_color1),\n        axis.text.y  = element_text(vjust=0.5, color = text_color1)\n       ) +\n  scale_y_continuous(labels = comma\n                     #, trans='log10'\n                    ) + \n  scale_x_continuous(breaks =  scales::pretty_breaks(n = 10))\n\ng"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["%r \n\ng <- \n  ggplot(data = temp %>% dplyr::filter(count<15), aes(x = count)) + \n  stat_count(fill = color1) +\n  theme.chart +\n  labs(title=\"Histogram of Number of Ratings by Users \\n(Long Tail Removed)\", \n       x = 'Number of Ratings by User',\n       y = 'Frequency') +\n  theme(axis.text.x  = element_text(vjust=0.5, color = text_color1),\n        axis.text.y  = element_text(vjust=0.5, color = text_color1)) +\n  scale_y_continuous(labels = comma) + \n  scale_x_continuous(breaks = scales::pretty_breaks(n = 16))\n\ng"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["As you can see most users rate very few products."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import collect_set\n\n# create temporary views for R to read from for visualizations\nmetaData = (attributesDF_newID\n            .join(categoriesDF_newID.groupBy('productId').agg(collect_set('categories').alias('categories')), \n                  'productId', 'fullouter')\n            .join(salesRankDF_newID.groupBy('productId').agg(collect_set('salesRank').alias('salesRank'))\n                  , 'productId', 'fullouter')\n            .join(relatedProductsDF_newID.groupBy('productId').agg(collect_set('related_productId').alias('related_productId'))\n                  , 'productId', 'fullouter')\n            #.sample(True, 0.0001)\n           )\n\n# print(metaData.count())\n# metaData.show(5)\n\nmetaData.createOrReplaceTempView(\"metaData\")"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"\nSELECT\n  title, \n  description\nFROM \n  metaData\nLIMIT \n  1000\n\")\n\nrdf <- collect(sparkdf)\n\nggplot_missing <- function(x){\n  \n  # Nicholas Tierney\n  # https://github.com/njtierney\n  \n  x %>% \n    is.na %>%\n    melt %>%\n    ggplot(data = .,\n           aes(x = Var2,\n               y = Var1)) +\n    geom_raster(aes(fill = value)) +\n    scale_fill_manual(name = \"\",\n                      labels = c(\"Data Present\",\"Data Missing\"), \n                      values = c(color3, bg_color1)) +\n    theme.chart +\n    theme(axis.text.x  = element_text(size = 16, vjust=0.5, color = text_color1),\n          axis.text.y  = element_text(size = 16, vjust=0.5, color = text_color1),\n          legend.background = element_rect(fill = bg_color1), \n          legend.text = element_text(size = 16, color = text_color1),\n          legend.position = \"right\"\n         ) + \n    labs(title = 'Missing-ness Diagram', \n         x = \"Variables\",\n         y = \"ProductId\")\n}\n\nggplot_missing(rdf)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["For our content based reccommenders, its important to see how much data is missing from potential features.  \nAs you can see, the situation isn't optimal. Description is often missing but we usually have the title.   \nThis tells us that a content based recommender approach along is not quite going to cut it.  \nInterestingly, we have pretty good image information. This makes the case for an image features based content reccommender."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import size\n\ntokenizer = (Tokenizer()\n             .setInputCol(\"description\")\n             .setOutputCol(\"words\")\n            )\n\ncontent_length = (tokenizer\n                  .transform(attributesDF_newID\n                             .filter(col('description').isNotNull())\n                             .filter(length(ltrim(col('description')))!=0))\n                  .withColumn('length_of_description', size('words'))\n                  .sample(False, 0.1, seed = seed)\n                 )\n\n# print(content_length.count()) #106,615 records\n\ncontent_length.createOrReplaceTempView(\"content_length\")"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# for character length\n# from pyspark.sql.functions import length\n\n# content_length = (attributesDF_newID\n#                   .withColumn('length_of_description', length('description'))\n#                   .sample(False, 0.01, seed = seed)\n#                  )\n\n# # print(content_length.count()) #23,254 records\n\n# content_length.createOrReplaceTempView(\"content_length\")"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%r\n\nsparkdf <- sql(\"SELECT * FROM content_length\")\n\nrdf <- collect(sparkdf)\n\n# some cleaning\ntemp <- rdf "],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%r \n\ng <- ggplot(data = temp %>% \n                     dplyr::filter(!is.na(length_of_description)) %>% \n                     dplyr::filter(length_of_description<3000)\n            , aes(x = length_of_description)) + \n  geom_histogram(fill = color1) +\n  theme.chart +\n  labs(title=\"Number of Words in Description\", \n       x = 'Number of Words',\n       y = 'Frequency') +\n  theme(axis.text.x  = element_text(angle = 90, vjust=0.5, color = text_color1),\n        axis.text.y  = element_text(vjust=0.5, color = text_color1)\n       ) +\n  scale_y_continuous(labels = comma) + \n  scale_x_continuous(breaks =  scales::pretty_breaks(n = 15))\n\ng"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%r \n\ng <- ggplot(data = temp %>% \n                     dplyr::mutate(!is.na(length_of_description)) %>% \n                     dplyr::filter(length_of_description<1000)\n            , aes(x = 1, y = length_of_description)) + \n  geom_violin(fill = color1) +\n  theme.chart +\n  labs(title=\"Length of Product Descriptions\", \n       x = '',\n       y = 'Number of Words') +\n  theme(axis.text.x  = element_blank(),\n        axis.text.y  = element_text(vjust=0.5, color = text_color1)\n       ) +\n  scale_y_continuous(labels = comma, breaks =  scales::pretty_breaks(n = 15))\n\ng"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["Most descriptions are less than 300 words long.  That's not a lot of data to build features on but as we'll see later, that is plenty enough to give us good results."],"metadata":{}},{"cell_type":"markdown","source":["## Baseline Model"],"metadata":{}},{"cell_type":"markdown","source":["One very simple way to recommend movies is to always recommend the movies with the highest average rating.  \nWe filter our products with high ratings for only those that have more than 1000 unique ratings (because products with few ratings may not have broad appeal)."],"metadata":{}},{"cell_type":"code","source":["highestRatedDF = (ratingsDF_complete\n                  .groupBy('productId')\n                  .agg(count(col('rating')).alias(\"count\")\n                       , avg(col('rating')).alias(\"average\"))\n                  .filter(\"count > 1000\")\n                  .join(attributesDF_newID, \n                       'productID'\n                       , 'left').orderBy(col(\"average\").desc())\n                 )\n\nhighestRatedDF = (highestRatedDF\n                  .withColumn(\"html\", \n                              concat(lit(\"<img style='height:300px;' src ='\"), col(\"imUrl\"), lit(\"'>\"))))\n\npictureOfBestFive = reduce(lambda row1,row2 : row1[:]+row2[:], highestRatedDF.select(\"html\").limit(5).collect())\n\ndisplayHTML(''.join(pictureOfBestFive))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["A way to enhance this is to recommend the highest rated products with more than 500 ratings in a given category."],"metadata":{}},{"cell_type":"code","source":["window = (Window\n          .partitionBy(['categories'])\n          .orderBy(col('average').desc())\n          )\n\nhighestRatedByCategoryDF = (ratingsDF_complete\n                            .join(categoriesDF_newID, 'productID', 'left')\n                            .groupBy('categories', 'productId')\n                            .agg(count(col('rating')).alias(\"count\")\n                                 , avg(col('rating')).alias(\"average\"))\n                            .filter(\"count > 500\")\n                            .join(attributesDF_newID, 'productID', 'left')\n                            .withColumn(\"rank\", rank().over(window).alias('rank'))\n                            .filter(\"rank <= 20\")\n                            .cache()\n                           )\n\ncategoriesList = [i.categories for i in highestRatedByCategoryDF.select('categories').distinct().collect()]\n\nfor category in categoriesList: \n  print(category)\n  highestRatedByCategoryDF.filter(col('categories')==category).show(5)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["These are our baseline models."],"metadata":{}},{"cell_type":"markdown","source":["## Content Based Recommender (N^2 approach)\n\n#### Brief overview of content based recommenders\n\nIf for example a user is currently viewing item A, one way of making a recommendation is to look for other items that are similar to Item A. Item B below shows a high similarity with Item A and therefore we can recommend item B back to the user. \n\n![Introduction_content_based](https://lh6.googleusercontent.com/ZU3NuSmMtDPLvkfWDX9omnWea18_cqnwrUpAbpeKG4OYzsn0hv-1-zOb4GSJWWJHqUBbXxKnuyg9AV4=w1366-h659\n \"image description\")\n\nFor more of the technical details, see below.  \n\n#### Brief overview of N^2 approach\n\nIn the N^2 approach, we compared every single product to every other product in our dataset to figure out which products are similar. This is computationally quite expensive and takes a very long time (in fact it never finished for us on the full dataset). For demonstration purposes we build this model on only a small subset of the data below.  \n\n![Introduction_N_squared](https://lh3.googleusercontent.com/2YOa0jnpB0UA4slOUEEKxLUzUbgqk0BFl80Vhsr81cnXF8PC4KtIAb2K41G7peXFAL-Mp_LIGdUFWh0=w1366-h659\n \"image description\")\n\nFor more of the technical details, see below.  \n\n#### Vector Space Model \n\nOur content based recommender system will use a relatively simple  model, known as the Vector Space Model (VSM) with basic TF-IDF weighting. VSM is essentially a spatial representation of text documents where each document is represented by a vector in a n-dimensional space and each dimension corresponds to a term from the overall vocabulary of a given document collection.  \n\nMore formally, every document is represented as a vector of term weights, where each weight indicates the degree of association between the document and the term. To make this vector, you need a set of documents D ={d1,d2,...,dN}, and a dictionary of all of the words in the corpus T ={t1,t2,...,tn}. Both of these can be easily obtained using various natural language processing (NLP) techniques, such as tokenization and stopwords removal. More advanced models can make use of NLP techniques like stemming and ngrams.    \n\nEach document dj is then represented as a vector in a n-dimensional vector space, so dj ={w1j,w2j,...,dnj}, where wkj is the weight for termt k in document dj. The most commonly used term weighting scheme is Term Frequency-Inverse Document Frequency weighting. TFIDF assumes words that occur frequently in one document (TF), but rarely in the rest of the corpus (IDF), are more likely to be relevant to the topic of the document. As an additional step, normalization of the resulting weight vectors prevents longer documents from having a better chance of retrieval.   \n\nContent based recommender systems relying on VSM will have both user proﬁles and items represented as weighted term vectors. Prediction of a user’s interest in a particular item can then be derived by computing the similarity between the user profile and the item. Cosine distance is a commonly used measure here.   \n\nOver the next few cells, we demonstrate an implementation of this methodology using Spark."],"metadata":{}},{"cell_type":"markdown","source":["Create a distinct dataset of productId and product description."],"metadata":{}},{"cell_type":"code","source":["# set to -1 to disable\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\n# udf_category ='Literature & Fiction'\nudf_category = 'Entrepreneurship'\n# udf_category = 'General'"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["# Test case\n# col('productId')==3229580, product_description should be null\n            \n# create a table of products and description\nproducts = (attributesDF_newID\n            # add categories data\n            .join(categoriesDF_newID, 'productID', 'left')\n            # filter for only one category for illustrative purposes\n            .filter(col('categories')==udf_category)\n            # combine titles and description\n            .withColumn('product_description', \n                        when(col('title').isNotNull() & col('description').isNotNull(), concat(col(\"title\"), col(\"description\")))\n                        .when(col('title').isNotNull() & col('description').isNull(), col(\"title\"))\n                        .when(col('title').isNull() & col('description').isNotNull(), col(\"description\"))\n                        .otherwise(lit(None))\n                       )\n            # only keep observations that have a product description \n            .filter(col('product_description').isNotNull())\n            .filter(length(ltrim(col('product_description')))!=0)\n            )\n\n# print('total number of rows = %s' %products.count())\nproducts.show(5)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["Tokenize the product\\_description, remove any stop words, stem words and create ngrams."],"metadata":{}},{"cell_type":"code","source":["# tokenize product description\ntokenizer = (Tokenizer()\n             .setInputCol(\"product_description\")\n             .setOutputCol(\"words\")\n            )\n\ntokenizedDF = (tokenizer\n               .transform(products)\n              )\n           \n# remove stop words \nremover = (StopWordsRemover()\n           .setInputCol(\"words\")\n           .setOutputCol(\"features\")\n          )\n\nnoStopWordsDF = (remover\n                 .transform(tokenizedDF)\n                )\n\n\nprint('total number of rows = %s' %noStopWordsDF.count())\nnoStopWordsDF.show(5)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# ngrams\n# from pyspark.ml.feature import NGram\n\n# ngram = (NGram()\n#          .setN(2)\n#          .setInputCol(\"words_filtered\")\n#          .setOutputCol(\"ngrams\")\n#         )\n\n# ngramDF = (ngram\n#            .transform(noStopWordsDF)\n#            .select('productId', col('ngrams').alias('features'))\n#           )"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["Apply TF-IDF."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer, IDF\n\n# Word count to vector for each wiki content\nvocabSize = 1000000\n\ncvModel = (CountVectorizer()\n           .setInputCol(\"features\")\n           .setOutputCol(\"tf\")\n           .setMinDF(5)\n           .setVocabSize(vocabSize)\n           .fit(noStopWordsDF)\n          )\n\n# Function to return True/False depending on if a sparseVector is not all zero or not \nisNoneZeroVector = udf(lambda v: v.numNonzeros() > 0, BooleanType())\n\nvectorizedDf = (cvModel\n                .transform(noStopWordsDF)\n                # filter out any rows where the features sparse vector is completely zero\n                .filter(isNoneZeroVector(col(\"tf\")))\n                # cache for performance\n                .cache()\n               )\n\n# print('total number of rows = %s' %vectorizedDf.count())\nvectorizedDf.show(5)"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["# compute IDF\nidf = (IDF(inputCol=\"tf\", outputCol=\"idf\", minDocFreq=2)\n       .fit(vectorizedDf)\n      )\n\ntfidf = idf.transform(vectorizedDf)\n\n# print('total number of rows = %s' %tfidf.count())\ntfidf.show(5)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["# sum TFIDF for all terms/(features) for each product/(row)\n#sum_ = udf(lambda v: float(v.values.sum()), DoubleType())\n\n#tfidf = (tfidf\n#         .withColumn(\"tf_sum\", sum_(\"tf\"))\n#         .withColumn(\"idf_sum\", sum_(\"idf\")))\n\n#print('total number of rows = %s' %tfidf.count())\n#tfidf.show(5)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Compute cosine similarity for each item against everything other item."],"metadata":{}},{"cell_type":"code","source":["compareDF = (tfidf\n             .select(col(\"productId\").alias(\"productId_a\"),\n                     col(\"description\").alias(\"description_a\"),\n                     col(\"imUrl\").alias(\"imUrl_a\"),\n                     col(\"idf\").alias(\"idf_a\"), \n                    )\n             # cartesian join to self\n             .crossJoin(tfidf\n                        .select(col(\"productId\").alias(\"productId_b\"),\n                                col(\"description\").alias(\"description_b\"),\n                                col(\"imUrl\").alias(\"imUrl_b\"),\n                                col(\"idf\").alias(\"idf_b\")))\n             #remove first row of easch group of asin as this is self\n             .filter(\"productId_a != productId_b\")\n             # cache for performance\n             #.persist(StorageLevel.OFF_HEAP)\n             .cache()\n            )\n\ncompareDF.show(5)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["import math\n\n# calculate dot product \ndotProd = udf(lambda a, b: float(a.dot(b)), DoubleType())\n\n# calculate norm\nnorm = udf(lambda v: float(math.sqrt(v.dot(v))), DoubleType())\n\n# calculate cosine similarity\ncosineSim = udf(lambda dot_product_ab, norm_a, norm_b: float(dot_product_ab / (norm_a * norm_b)), DoubleType())\n\nsimilarityDF = (compareDF\n                # cosine similarity calculation\n                .withColumn('dot_product_ab', dotProd(col('idf_a'), col('idf_b')))\n                .withColumn('norm_a', norm(col('idf_a')))\n                .withColumn('norm_b', norm(col('idf_b')))\n                .withColumn('similarity', cosineSim(col('dot_product_ab'), col('norm_a'), col('norm_b')))\n                .cache()\n               )\n\n#print('total number of rows = %s' %similarityDF.count())\nsimilarityDF.show(5)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["#### Evaluation\n\nFirst, let's do a visual comparison of the covers of the top most similar products."],"metadata":{}},{"cell_type":"code","source":["window = (Window\n          .partitionBy(['productId_a'])\n          .orderBy(col('similarity').desc())\n          )\n\nsimilarProductsDF = (similarityDF\n                     # reduce dataset size for faster processing\n                     .filter(col('similarity')>0.8)\n                     # for each product_a, keep only the row with the most similar product_b\n                     .withColumn(\"rank\", rank().over(window).alias('rank'))\n                     .filter(col('rank')==1)\n                     # rank across the whole dataset\n                     .orderBy(col('similarity').desc())\n                     .coalesce(1)\n                     .withColumn('id', monotonically_increasing_id())\n                     #.cache()\n                     )\n\n# similarProductsDF.show(5)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["def visualize_similar_products(df, id):\n    \"\"\"\n    Args:\n        df (sparkdf): a dataframe with columns id, imurl_a, imurl_b\n        id (integer): id of row\n        imurl_a (string): image link of product a\n        imurl_b (string): image link of product b\n\n    Returns:\n        Returns images of products in id row.\n    \"\"\"\n    pictures = (df\n                .filter(col('id')==id)\n                .withColumn(\"html_a\", concat(lit(\"<img style='height:300px;' src ='\"), col(\"imUrl_a\"), lit(\"'>\")))\n                .withColumn(\"html_b\", concat(lit(\"<img style='height:300px;' src ='\"), col(\"imUrl_b\"), lit(\"'>\")))\n                .select(\"html_a\", \"html_b\")\n               )\n\n    picture1 = pictures.select(\"html_a\").collect()[0][0]\n    picture2 = pictures.select(\"html_b\").collect()[0][0]\n    pictures_temp = picture1 + picture2\n\n    return displayHTML(pictures_temp)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF, 1)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF, 2)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF, 3)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF, 4)"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF, 5)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":["As you can see our content reccommendation engine does a good job of finding similar products. In fact, in one sense, it does too good a job and finds duplicate products in the dataset, i.e. the same product with different productIds. To avoid this, one would need to use a deduplicated dataset or avoid reccommending products that are too similar (e.g. only reccommend products with cosine similarity <0.98).   \n\nWe'd like to evaluate our results at a macro level. For this, we will use the \"related products\" data we got from the meta dataset. By comparing the products our model says are similar to products that are often bought/viewed together (this will be our gold\\_standard), we can see how well our model is doing. One thing to bear in mind however is that we do not know what the threshold was for determining is a set of products can be considered as \"often bought together\" \"often viewed together.\" This implies that there may be more pairs of products that we could compare to if the threshold was set too high."],"metadata":{}},{"cell_type":"code","source":["gold_standard = (relatedProductsDF_newID\n                 .select(col('productId').alias('productId_a'), \n                         col('related_productId').alias('productId_b')\n                        )\n                 .distinct()\n                 # keep only entrepeneurship for product_a\n                 .join(categoriesDF_newID.select(col('productId').alias('productId_a'), \n                                                 col('categories').alias('categories_a'))\n                       , 'productId_a', 'left')\n                 .filter(col('categories_a')==udf_category)\n                 # keep only entrepeneurship for product_b\n                 .join(categoriesDF_newID.select(col('productId').alias('productId_b'), \n                                                 col('categories').alias('categories_b'))\n                       , 'productId_b', 'left')\n                 .filter(col('categories_b')==udf_category)\n                 # identify as true recommendation\n                 .withColumn('Truth', lit(1))\n                 # select final columns\n                 .select('productId_a', 'productId_b', 'Truth')\n                 .cache()\n                )\n\n# print(gold_standard.count())\ngold_standard.show(5)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["gold_standard_predictions = (compareDF\n                             .select('productId_a', 'productId_b')\n                             # add data on which which products are truly bought/viewed together\n                             .join(gold_standard, ['productId_a', 'productId_b'], 'left')\n                             .withColumn('Truth', \n                                         when(col('Truth').isNull(), 0)\n                                         .otherwise(col('Truth')))\n                             # add similarity data from VSM model \n                             .join(similarityDF.select('productId_a', 'productId_b', 'similarity')\n                                   , ['productId_a', 'productId_b'], 'left')\n                             # cache temporarily\n                             .cache()\n                            )\n\n#print(gold_standard_predictions.count())\ngold_standard_predictions.show(5)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["# range over which to test\nthresholds = np.arange(0, 1.05, 0.05)\n\n# initialize empty dataframe\nresults = pd.DataFrame([])\n\nfor i in np.arange(0, len(thresholds)):\n  \n  threshold = thresholds[i]\n  \n  temp = (gold_standard_predictions\n          .withColumn('Threshold', lit(threshold))\n          .withColumn('Prediction', \n                      when(col('similarity')>threshold, 1)\n                      .otherwise(0)\n                     )\n          .withColumn('Class', \n                      when((col('Truth')==1) & (col('Prediction')==1), 'TP')\n                      .when((col('Truth')==0) & (col('Prediction')==1), 'FP')\n                      .when((col('Truth')==0) & (col('Prediction')==0), 'TN')\n                      .when((col('Truth')==1) & (col('Prediction')==0), 'FN')\n                      .otherwise(None)\n                     )\n          .groupBy('Threshold', 'Class')\n          .count()\n         )\n  \n  results = results.append(temp.toPandas())\n  \n# gold_standard_predictions.unpersist()\n\nresults"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["# create some helper functions\ndef precision(row):\n  # Precision = true-positives / (true-positives + false-positives)\n  result = row['TP'] / (row['TP'] + row['FP']) \n  result = float(result)\n  return result\n\ndef recall(row):\n  # Recall = true-positives / (true-positives + false-negatives)\n  result = row['TP'] / (row['TP'] + row['FN']) \n  result = float(result)\n  return result\n\ndef f_measure(row):\n  # F-measure = 2 x Recall x Precision / (Recall + Precision)\n  result = (2 * row['recall'] * row['precision']) / (row['recall'] + row['precision'])\n  result = float(result)\n  return result"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["temp = (results\n           .pivot(index='Threshold', columns='Class', values='count')\n           .fillna(value = 0)\n           .reset_index()\n          )\n\n# apply functions\ntemp['precision'] = temp.apply(lambda row: precision(row), axis=1)\ntemp['recall'] = temp.apply(lambda row: recall(row), axis=1)\ntemp['f_measure'] = temp.apply(lambda row: f_measure(row), axis=1)\n\n# see results\ntemp"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["display(temp.plot.line(x = 'Threshold', y = ['precision', 'recall', 'f_measure']).figure)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["# for presentation slides\n# my_colors = [(0,0.27,0.67), (0.51,0.78,0.65), (0.93,0.81,0.10)]\n\n# ax = temp.plot.line(x = 'Threshold', y = ['precision', 'recall', 'f_measure'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":["While these results aren't very impressive, even this level of performance indicates that this model is clearly working (albeit at a low level). Moreover, you have to bear in mind that the fact that if a customer saw product A and didn't see product B, that  does not necessarily mean that the customer is not interested in product B.  \n\nLeaving aside the caveats, we can't expect excellent performance from a basic model like this. There are many ways to boost the performance here, e.g. building a model that is able to understand semantics."],"metadata":{}},{"cell_type":"markdown","source":["#### Conclusions\n\nAnother use-case for our VSM model, given the results we're seeing, is as a deduplication algorithm.    \n\nBased on our precision, recall and f-score curves, another conclusion we can draw is that product similarity is not necessarily the best way to make reccommendations. This makes intuitive sense when you consider cases where a customer may be looking to use the reccommendation engine as a discovery tool or when a customer is looking for reccommendations for complementary products to (e.g. a plate to go with a set of knives). In both these cases, this model would fail to give the desired reccommendations.  \n\nWhile the N^2 approach to finding similar items is certainly accurate, it is ultimately too time-consuming, volume-intensive, and hardware-reliant for any scalable purpose. In fact, we were never able to estimate the time it would take to run this on the full books dataset available to us as our moderately powered cluster could not complete the computations. In the next section, we look at an alternative approach which sacrifices a little accuracy for speed.   \n\nOther approaches we considered included using other features to reduce the number of comparisons. For example, you could use pricing data to compare only products in the same price range. The downside here is you end up limiting upselling oppportunities. A better candidate variable might have been the Category variable which could be used to compare only products in the same category. Unfortunately, this condition is not restrictive enough to reduce the number of comparisons to a level our cluster could handle."],"metadata":{}},{"cell_type":"markdown","source":["## Content Based Reccommender (LSH approach)\n\nThere are a number of methods for addressing the disadvantages of our previous approach. One of the most promising methods is involves a combination of MinHash functions, Locality Sensitive Hashing (or LSH) and Nearest Neighbour Search, which we were inspired to try after learning of [Uber's success](https://databricks.com/blog/2017/05/09/detecting-abuse-scale-locality-sensitive-hashing-uber-engineering.html) using this approach to detect fraudulent activity at scale.  \n\n#### Brief overview of Locality Sensitive Hashing \n\nThe gist of it is that we use the LSH algorithm to \"bucket\" items together and then compare only items in the same bucket. In the example below, you would compare only the circle and the cross items as they are in the same \"bucket.\" In this way, you are able to drastically reduce the number of operations as compared to the N^2 approach (above). This is what we were looking for, something that is highly scalable and extremely fast. \n\n![Intro_lsh](https://lh4.googleusercontent.com/4PoFVvfWkW81lpTUS3gW9PNSRi3kA_Ax9ryqYim1Wg66x1XnnWIZc6iq32dqNTZwHUY7eoYzj4yZ3vE=w1366-h659\n \"image description\")\n\nFor more details on the technicalities behind this approach, see below.  \n\n#### MinHash  \n  \nThe MinHash function involves using a collection of hash functions, where we evaluate each word of a document by inputting it into the N hash functions, producing N hashed values. We then choose the minimum hash value which represents the signature for the word out of the N generated hash values. In this case, the minimum function is the function that samples the hashes and chooses the representation. The resulting set of hashes of words is comprised of the minimum hash computed for a word which was chosen from the N hash functions. The resulting vector of minimum hash values is what we call the MinHash signature of the document. We choose the minimum hash value by convention and for simplicity more than anything. We could just as well choose the maximum hash value as the sampling signature, the decision is arbitrary. Whatever we choose though, the hash values needs to be principled and consistent.  \n\n#### Locality Senstive Hashing\n\nLocality Sensitive Hashing is an algorithm which samples the result of the MinHash algorithm and compresses the MinHash signatures into LSH buckets. This serves to further reduce the size of the number of features that need to be compared to determine if documents are candidates for being similar. The idea behind LSH is that if documents are similar they should hash to approximately the same value. So, given some similarity threshold and N hash functions, sample the MinHash function in such a way that two documents are candidate pairs for similarity if and only if at least one of their LSH buckets are identical and share the same offset within the signature.  \n\nNote that LSH allows us to quickly compare documents that are potential candidate matches. However, we could just use the MinHash signatures and compare those values to determine similarity. However, if we don’t use LSH to give us candidate pairs for matching, we would need to compare all MinHash signatures of our documents to all of the other documents that have been MinHashed, which would take order O(n2) number of operations, the same problem we had before. If we treat LSH values as buckets, then we can determine potential candidate pairs in order O(n) time by binning those LSH values that match together, and only if two documents have the same LSH bin would we further compute the MinHash similarity.   \n\nOver the next few cells, we demonstrate an implementation of this methodology using Spark. We already have the basic dataset vectorizedDf ready so we can go straight from there."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import MinHashLSH\n\nmhlsh = (MinHashLSH()\n         .setNumHashTables(100)\n         .setInputCol(\"tf\")\n         .setOutputCol(\"hashValues\")\n         .fit(vectorizedDf)\n#          .persist(StorageLevel.OFF_HEAP)\n        )\n\n      \n#mhlsh.transform(vectorizedDf).show(5)"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["approxSimilarityJoin"],"metadata":{}},{"cell_type":"code","source":["# threshold is based on Jaccard distance (not Jaccard similarity)\n# the higher the Jaccard distance between two objects, the less similar they are\nthreshold = 0.9\n\nsimilarityDF_lsh = (mhlsh\n                    .approxSimilarityJoin(vectorizedDf, vectorizedDf, threshold)\n                    .filter(\"distCol != 0\")\n                    .filter(col('datasetA.productId')!=col('datasetB.productId'))\n                    .select(col('datasetA.productId').alias('productId_a')\n                            , col('datasetB.productId').alias('productId_b')\n                            , col('distCol').alias('similarity')\n                           )\n                    .persist(StorageLevel.OFF_HEAP)\n                   )\n\n# print(similarityDF_lsh.count())\n# similarityDF_lsh.show(5)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":["#### Evaluation  \nLet's start off by visualizing some of the most similar products."],"metadata":{}},{"cell_type":"code","source":["tempMetaData_a = (attributesDF_newID\n                  .select(col('productId').alias('productId_a'), \n                          col('description').alias('description_a'),\n                          col('imUrl').alias('imUrl_a')\n                         )\n                 )\n\ntempMetaData_b = (attributesDF_newID\n                  .select(col('productId').alias('productId_b'), \n                          col('description').alias('description_b'), \n                          col('imUrl').alias('imUrl_b')\n                         )\n                 )  \n\n\nwindow = (Window\n          .partitionBy(['productId_a'])\n          .orderBy(col('similarity').asc())\n          )\n\nsimilarProductsDF_lsh = (similarityDF_lsh\n                         .filter(col('similarity')>0.5)\n                         .withColumn(\"rank\", rank().over(window).alias('rank'))\n                         .filter(col('rank')==1)\n                         .orderBy(col('similarity').asc())\n                         .coalesce(1)\n                         .withColumn('id', monotonically_increasing_id())\n                         # add description for productId_a\n                         .join(tempMetaData_a, \n                               'productId_a', \n                               'left')\n                         # add description for productId_b\n                         .join(tempMetaData_b, \n                               'productId_b', \n                               'left')\n                        )\n\nsimilarProductsDF_lsh.show(5)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF_lsh, 1)"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF_lsh, 2)"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF_lsh, 3)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF_lsh, 4)"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["visualize_similar_products(similarProductsDF_lsh, 5)"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":["As we saw with the N^2 approach, our model does too good a job and finds duplicate products in the dataset."],"metadata":{}},{"cell_type":"markdown","source":["##### Perfomance comparison of N^2 and LSH"],"metadata":{}},{"cell_type":"code","source":["# performance comparison in terms of running time for Entrepreneurhsip category\n\n"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"markdown","source":["#### Content Based Demo"],"metadata":{}},{"cell_type":"code","source":["# create a table of products and description\nproducts = (attributesDF_newID\n            # add categories data\n            .join(categoriesDF_newID, 'productID', 'left')\n            # filter for only one category for illustrative purposes\n            #.filter(col('categories')==udf_category)\n            # combine titles and description\n            .withColumn('product_description', \n                        when(col('title').isNotNull() & col('description').isNotNull(), concat(col(\"title\"), col(\"description\")))\n                        .when(col('title').isNotNull() & col('description').isNull(), col(\"title\"))\n                        .when(col('title').isNull() & col('description').isNotNull(), col(\"description\"))\n                        .otherwise(lit(None))\n                       )\n            # only keep observations that have a product description \n            .filter(col('product_description').isNotNull())\n            .filter(length(ltrim(col('product_description')))!=0)\n            )"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"markdown","source":["Tokenize the product\\_description amd remove any stop words."],"metadata":{}},{"cell_type":"code","source":["# tokenize product description\ntokenizer = (Tokenizer()\n             .setInputCol(\"product_description\")\n             .setOutputCol(\"words\")\n            )\n\ntokenizedDF = (tokenizer\n               .transform(products)\n              )\n           \n# remove stop words \nremover = (StopWordsRemover()\n           .setInputCol(\"words\")\n           .setOutputCol(\"features\")\n          )\n\nnoStopWordsDF = (remover\n                 .transform(tokenizedDF)\n                )"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":["Calculate TF"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer, IDF\n\n# Word count to vector for each wiki content\nvocabSize = 1000000\n\ncvModel = (CountVectorizer()\n           .setInputCol(\"features\")\n           .setOutputCol(\"tf\")\n           .setMinDF(5)\n           .setVocabSize(vocabSize)\n           .fit(noStopWordsDF)\n          )\n\n# Function to return True/False depending on if a sparseVector is not all zero or not \nisNoneZeroVector = udf(lambda v: v.numNonzeros() > 0, BooleanType())\n\nvectorizedDf_demo = (cvModel\n                     .transform(noStopWordsDF)\n                     # filter out any rows where the features sparse vector is completely zero\n                     .filter(isNoneZeroVector(col(\"tf\")))\n                     # cache for performance\n                     .cache()\n                    )\n\n# print('total number of rows = %s' %vectorizedDf.count())\n# vectorizedDf.show(5)"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"markdown","source":["Train model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import MinHashLSH\n\nmhlsh = (MinHashLSH()\n         .setNumHashTables(100)\n         .setInputCol(\"tf\")\n         .setOutputCol(\"hashValues\")\n         .fit(vectorizedDf_demo)\n        )"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"markdown","source":["Select a random productId"],"metadata":{}},{"cell_type":"code","source":["# select a random productId \n# seed = 2 is another great example\nudf_productId = vectorizedDf_demo.select('productId').rdd.takeSample(False, 1, seed = seed)[0][0]\nudf_productId"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["udf_productId = 89203\n# udf_productId = 168082"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["udf_product_info = (vectorizedDf_demo\n                    .filter(col('productId')==udf_productId)\n                   )\n\n# udf_product_info.show()\n\ncolumns = ['productId', 'title', 'description', 'price', 'imUrl']\n\nfor column in columns:\n  print(column + ': ' + str(udf_product_info.select(column).collect()[0][0]))\n  print('')"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["imUrl_list = udf_product_info.select('imUrl').collect()\nlinks = [str(i.imUrl) for i in imUrl_list]\n\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in links]\n\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"markdown","source":["Perform a nearest neighbour search for that ProductId"],"metadata":{}},{"cell_type":"code","source":["key = (vectorizedDf_demo\n       .filter(col('productId')==udf_productId)\n       .select('tf')\n       .collect()[0][0]\n      )\n\nk = 10\n\nsimilar = mhlsh.approxNearestNeighbors(vectorizedDf_demo, key, k).orderBy(col('distCol'))\n\nsimilar.select('productId', 'title', 'brand', 'description', 'price', 'distCol').show(k)"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["imUrl_list = (similar\n              .filter(col('productId')!=udf_productId)\n              .select('imUrl')\n              .collect()\n             )\n\nlinks = [str(i.imUrl) for i in imUrl_list]\n\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in links]\n\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"markdown","source":["#### Conclusion \n\n1. We were successfully able to decrease the training time for our content based recommender model by applying LSH techinques.  \n2. Our evaluation results with the content based reccommenders showed clear signs that the model were making useful recommendations. While we do note that there is siginificant room for improvement, a content based approach can only go so far. Making reccommendations solely on the basis of similar products is not necessarily a wise strategy. Consumers often are not looking for similar products but rather products that are different but that they might still be interested in. To make these kinds of reccommendations, we have to use other methods. One such method is collaborative filtering, which we explore in the next section."],"metadata":{}},{"cell_type":"markdown","source":["## Collaborative Filtering with Alternating Least Squares\n\n#### A brief introduction to Collaborative Filtering\n\nOne method of collaborative filtering works by looking for similar users.   \nIn the example below, you can see user A is very similar to user C in that they like very similar items. \nIn this sense, it is highly likely that if we were recommend Item 5 to user A, they would probably like it. \n\n![Amazon_home_page_mock](https://lh6.googleusercontent.com/vqrjEvrGtiqgUBSO72RrHPH4sucoR4JwRDirJtt97X_jziqU3q0x81pES19MfxGZ2t6Sec6pfeOtqec=w1366-h659\n \"image description\")\n\nNext up we discuss matrix factorization which is pretty fundamental to building a collaborative filtering model.  \n\n#### Matrix Factorization \n\nWe can represent our data as a (very) large matrix A, say of dimensions m x n. where each row of A represents a user (m users in total), and each columns of A corresponds to an item (n items in total). Naturally, this matrix will be quite sparse (as we have shown previously) since most users will have only bought a very small fraction of all (n) items in the dataset. So we will have that Aij (i.e. the (i,j)th entry of the matrix) will depict the the rating the ith user gave the jth item.  \n\nThe goal of Matrix Factorization models is to approximate A with two smaller matrices, U (k x m) and V (k x n), each of which represent the rows and columns of A respectively (users and items). The vectors ui and vi in U and V respectively are called \"latent factors,\" and k represents the number of features we believe associate each user to the item. Each entry in the row vectors of U and V therefore expresses how much association each has with k features.  \n\nWe then predict the “rating” of user U of item V to be:  \nri = uTi vj  \n\nAnd so our aim to is to approximate (and complete) A as follows:  \nA ≈ UT V  \n\nWe say that we are completing matrix A since we start with a sparse matrix A, but product the UTV will yield a dense, matrix.  \n\nOur attention now switches to find the matrices U and V that best approximate A.  \n\n#### Alternating Least Squares Algorithm  \n\nOne approach to finding U and V is ALS.  \n\nWe want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The Alternating Least Squares algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the items matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.  \n\nOver the next few cells, we demonstrate an implementation of this methodology using Spark."],"metadata":{}},{"cell_type":"markdown","source":["#### Data preparation\nCreate training, validation and testing datasets."],"metadata":{}},{"cell_type":"code","source":["(split_60_df, split_a_20_df, split_b_20_df) = ratingsDF_complete.randomSplit([0.6, 0.2, 0.2], seed)\n\ntraining_df = split_60_df.cache()\nvalidation_df = split_a_20_df.cache()\ntest_df = split_b_20_df.cache()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"markdown","source":["#### Build model\nWe use the training_df and validation_df to build a cross-validated model.  \n\nThe CrossValidator is capable to running the ALS Algorithm using a range of Regularization Parameters and, Rank in order to provide an ALS model which yeild the lowest error (RMSE). Although we could certainly use the model provided by the CrossValidator, it would be difficult to rebuild a model with the same parameter without rerunning the CrossValidator. In order to be able to rebuild the model on-demand, we decided to search over pairs of Regularization Parameters and Ranks in order to find the Parameters that yeild the lowest error (RMSE)."],"metadata":{}},{"cell_type":"code","source":["# # Let's initialize our ALS learner\n# als = ALS()\n\n# # Now we set the parameters for the method\n# als.setMaxIter(10)\\\n#    .setSeed(seed)\\\n#    .setRegParam(0.001)\\\n#    .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\")\n  \n\n# # Create an RMSE evaluator using the label and predicted columns\n# reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n\n# tolerance = 0.03\n# ranks = [4, 8, 12]\n# errors = [0, 0, 0]\n# models = [0, 0, 0]\n# err = 0\n# min_error = float('inf')\n# best_rank = -1\n\n# for rank in ranks:\n#   # Set the rank here:ratingsDf\n#   als.setParams(rank=rank)\n#   # Create the model with these parameters.\n#   model = als.fit(training_df)\n#   # Run the model to create a prediction. Predict against the validation_df.\n#   predict_df = model.transform(validation_df)\n\n#   # Remove NaN values from prediction (due to SPARK-14489)\n#   predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n#   # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n#   error = reg_eval.evaluate(predicted_ratings_df)\n#   errors[err] = error\n#   models[err] = model\n#   print 'For rank %s the RMSE is %s' % (rank, error)\n#   if error < min_error:\n#     min_error = error\n#     best_rank = err\n#   err += 1\n\n# als.setRank(ranks[best_rank])\n# print 'The best model was trained with rank %s' % ranks[best_rank]\n# my_model = models[best_rank]"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["# # We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n# # Let's create our CrossValidator with 3 fold cross validation\n# crossval = CrossValidator(estimator=als, evaluator=reg_eval, numFolds=3)\n\n# # Let's tune over our regularization parameter from 0.01 to 0.10\n# regParam = [0.1,0.01,0.001,0.0001]\n# rank = [2, 4, 8, 12, 24]\n\n# # We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\n# paramGrid = (ParamGridBuilder()\n#              .addGrid(als.regParam, regParam)\n#              .addGrid(als.rank, rank)\n#              .build())\n# crossval.setEstimatorParamMaps(paramGrid)\n\n# # Now let's find and return the best model\n# cvModel = crossval.fit(training_df).bestModel\n\n# predict_df = cvModel.transform(validation_df)\n\n# # Remove NaN values from prediction (due to SPARK-14489)\n# predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n# # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n# error = reg_eval.evaluate(predicted_ratings_df)\n\n# print error"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["# predict_df.show(100)\n# usersCount = rawRatingsDF.select(\"reviewerId\").distinct().count()\n# productId = rawRatingsDF.select(\"asin\").distinct().count()\n\n# print ratingsDF.select(\"userId\").distinct().count()\n# print usersCount\n# print ratingsDF.select(\"productId\").distinct().count()\n# print productId\n# print ratingsDF.select(\"rating\").distinct().count()"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":["### Choosing Hyperparameters using learning curves\n\nOverfitting is the term used to describe the situation where a statistical model describes random error or noise instead of the underlying relationship being modelled. This usually results in poor real-life performance, as can be simulated on our testing datasets. To overcome this challenge, we used learning curves to gauge the optimal hyperparameters for the ALS algorithm that would give us a model that neither overfits nor underfits.  \n\nAmong other things, a learning curve basically allows you to find the point at which the algorithm starts to learn. We generated our learning curves using the following process:  \n1. Train ALS model on a 5% subset of the training dataset \n2. Calculate RMSE on testing dataset \n3. Calculate RMSE on subset of the training dataset \n4. Repeat from step 1 except increase subset of the training dataset by 5%.  \n\nThe code for this process is commented out below due to long execution time."],"metadata":{}},{"cell_type":"code","source":["# # ParamGrid\n\n# # Code commented out - long execution\n\n# regParams = [0.8, 0.5, 0.3, 0.1, 0.05, 0.01]\n# ranks = [8, 9, 10, 11, 12]\n# ranges = [[0.05, 0.95], [0.1, 0.9], [0.15, 0.85], [0.2, 0.8], [0.25, 0.75], [0.3, 0.7], [0.35, 0.65], [0.4, 0.6], [0.45, 0.55], [0.5, 0.5], [0.55, 0.45], [0.6, 0.4], [0.65, 0.35], [0.7, 0.3], [0.75, 0.25], [0.8, 0.2], [0.85, 0.15], [0.9, 0.1], [0.95, 0.05]]\n\n# def randomSplitTrainTestVal(df, rng):\n#   (trn, tst) = df.randomSplit(rng, seed)\n#   train = trn.cache()\n#   test = tst.cache()\n#   return (train, test)\n  \n# def get_train_test_errors(model, reg_eval, train, test):\n#   predict_df = model.transform(test)\n#   predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n#   test_error = reg_eval.evaluate(predicted_ratings_df)\n#   train_predict_df = model.transform(train)\n#   train_predicted_ratings_df = train_predict_df.filter(train_predict_df.prediction != float('nan'))\n#   train_error = reg_eval.evaluate(train_predicted_ratings_df)\n#   return (train_error, test_error)\n\n# def get_learning_curve(df, ranges, rank, regParam):\n#   (train, test) = randomSplitTrainTestVal(df, [0.8, 0.2])\n#   errors = []\n#   reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n#   als = (ALS()\n#            .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\")\n#            .setMaxIter(10)\n#            .setSeed(seed)\n#            .setRegParam(regParam)\n#            .setParams(rank=rank))\n#   for r in ranges:\n#     # print \"range: \", r\n#     (train_subset, other) = randomSplitTrainTestVal(train, r)\n#     model = als.fit(train_subset)\n#     (train_error, test_error) = get_train_test_errors(model, reg_eval, train_subset, test)\n#     # print \"train error\", train_error, \"test error\", test_error\n#     errors.append((train_error, test_error))\n#   # 100% training set\n#   # print \"range: \", [1.0, 0.0]\n#   (train_error, test_error) = get_train_test_errors(model, reg_eval, train_subset, test)\n#   # print \"train error\", train_error, \"test error\", test_error\n#   errors.append((train_error, test_error))\n#   return errors\n\n# for regParam in regParams:\n#   for rank in ranks:\n#     print \"STARTED  rank:\", rank, \"regParam:\", regParam\n#     lc = get_learning_curve(reviewsDF_newID, ranges, rank, regParam)\n#     print \"FINISHED rank\", rank, \"regParam\", regParam\n#     print \"lc: \", lc\n#     print \"-----\""],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"markdown","source":["Based on the results of the previos code block, we chose the following values for our tunable hyperparameters:   \n- regularization = 0.3\n- rank = 8\n\nWe believe these values achieve the right balance of model complexity and performance.  \nThe following graphs show the performance of our model with these hyper-parameters on each of the different data sets."],"metadata":{}},{"cell_type":"code","source":["labels = []\nx = []\nfor i,rng in enumerate(range(19)):\n  labels.append(str(int((i+1) * 0.05 * 100)) + \"%\")\n  x.append(i)\nprint labels\nprint x\n\nreviews_lc = [(0.8640580121589105, 4.384441927740864), (0.9162809231699517, 3.690484300482888), (0.9384299226557588, 3.0612438047015575), (0.9466430715381681, 2.360932502568802), (0.9692083871156905, 1.9229308523437973), (0.9903098247046015, 1.7327298344878108), (1.010487906427418, 1.5416729229638737), (1.02603529745848, 1.4531459751437417), (1.0413454194101428, 1.378061221149991), (1.0525018106038844, 1.3369410801994481), (1.058597756535187, 1.326793759852507), (1.0712226906806182, 1.2877614927599652), (1.078651407565941, 1.27308252851925), (1.0878179342151915, 1.2561668513171094), (1.0921919895577556, 1.2476713431206847), (1.0970279362647135, 1.2421737543609286), (1.1020407600642035, 1.2333738897994269), (1.1046411136327223, 1.2302246933071448), (1.1094361605595275, 1.2251588329161107), (1.1094361605595278, 1.2251588329161107)]\n\nplt.close()\nfigure = plt.subplot()\nplt.plot(reviews_lc)\nfigure.set_ylim((0,5))\nfigure.set_xlim((-1,20))\nplt.xlabel('Iterations')\nplt.ylabel('RMSE')\nplt.title('Learning Curve - Reviews Only')\nplt.xticks(x, labels, rotation='vertical')\nplt.grid(False)\ntest_legend = mpatches.Patch(color='green', label='Test')\ntraining_legend = mpatches.Patch(color='blue', label='Training')\nplt.legend(handles=[test_legend, training_legend])\nplt.show()\ndisplay()\n"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["# # for presentation\n# temp = pd.DataFrame([])\n\n# for i in np.arange(0, len(reviews_lc)):\n  \n#   temp = temp.append(pd.DataFrame({'% of Training Data Used': (i+1)*5, \n#                                    'Training Error': reviews_lc[i][0],\n#                                    'Testing Error': reviews_lc[i][1]\n#                                   }\n#                                   , index = [i]\n#                                  )\n#                     )\n\n# temp \n\n\n# my_colors = [(1,1,1), (0.93,0.81,0.10), (0.51,0.78,0.65), (0,0.27,0.67)]\n\n# ax = temp.plot.line(x = '% of Training Data Used', y = ['Training Error', 'Testing Error'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# ax.legend(loc='best', fancybox=False, framealpha=0.2)\n\n# ax.set_ylim((0,5))\n# # figure.set_xlim((-1,20))\n\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["ratings_lc = [(0.8512420488592438, 4.402178452360909), (0.8985128906858578, 4.0079660176590926), (0.8946599135995017, 3.3939064738820286), (0.8985310810979403, 2.9684426207134504), (0.9089132610834753, 2.7348769794625443), (0.9194935511451728, 2.5877722971335286), (0.9287053424392893, 2.4157057608007646), (0.9366717124747126, 2.252721805815866), (0.9447478338776744, 2.116982703636849), (0.952174018424869, 2.05916611810784), (0.9594409377306761, 2.0338574307914956), (0.9661796957765452, 1.9545542418518507), (0.9738437348248564, 1.8432566433493862), (0.9788196624337752, 1.8132234200019504), (0.9847788245948345, 1.7718882306687063), (0.9892288532410219, 1.7468879562807988), (0.994331713485135, 1.7082346522186365), (0.9980104983233303, 1.7028764604276119), (1.0021646699077813, 1.6776681678401146), (1.0021646699077813, 1.6776681678401142)]\n\nplt.close()\nfigure = plt.subplot()\nplt.plot(ratings_lc)\nfigure.set_ylim((0,5))\nfigure.set_xlim((-1,20))\nplt.xlabel('Iterations')\nplt.ylabel('RMSE')\nplt.title('Learning Curve - Ratings Only')\nplt.xticks(x, labels, rotation='vertical')\nplt.grid(False)\ntest_legend = mpatches.Patch(color='green', label='Test')\ntraining_legend = mpatches.Patch(color='blue', label='Training')\nplt.legend(handles=[test_legend, training_legend])\nplt.show()\ndisplay()\n"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["# # for presentation\n# temp = pd.DataFrame([])\n\n# for i in np.arange(0, len(ratings_lc)):\n  \n#   temp = temp.append(pd.DataFrame({'% of Training Data Used': (i+1)*5, \n#                                    'Training Error': ratings_lc[i][0],\n#                                    'Testing Error': ratings_lc[i][1]\n#                                   }\n#                                   , index = [i]\n#                                  )\n#                     )\n\n# temp \n\n\n# my_colors = [(1,1,1), (0.93,0.81,0.10), (0.51,0.78,0.65), (0,0.27,0.67)]\n\n# ax = temp.plot.line(x = '% of Training Data Used', y = ['Training Error', 'Testing Error'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# ax.legend(loc='best', fancybox=False, framealpha=0.2)\n\n# ax.set_ylim((0,5))\n# # figure.set_xlim((-1,20))\n\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["complete_lc = [(0.851692687540396, 4.428562988682532), (0.8888863926069245, 3.880530106413204), (0.8874734804981644, 3.291523523610593), (0.8989707921588207, 2.9918748961421504), (0.9188548437487172, 2.9241459476195453), (0.918096453656919, 2.5138877808372357), (0.9275799356490655, 2.3889831310562513), (0.9360103065779821, 2.2529617285635153), (0.9455891898283856, 2.2464164794173667), (0.9520339232268076, 2.1076277324519577), (0.9596608474711091, 1.9912922342326205), (0.9662722305220206, 1.9273690722135204), (0.972105789686793, 1.8703246703666923), (0.9808310056738392, 1.7909899312840658), (0.9840512588991773, 1.7788630634497657), (0.988626393352775, 1.7679194184820883), (0.9928639045269848, 1.7391293758274775), (0.9981451674753056, 1.693672279285886), (1.0029811023548745, 1.658773456140052), (1.0029811023548745, 1.658773456140052)]\n\nplt.close()\nfigure = plt.subplot()\nplt.plot(complete_lc)\nfigure.set_ylim((0,5))\nfigure.set_xlim((-1,20))\nplt.xlabel('Iterations')\nplt.ylabel('RMSE')\nplt.title('Learning Curve - Ratings and Reviews')\nplt.xticks(x, labels, rotation='vertical')\nplt.grid(False)\ntest_legend = mpatches.Patch(color='green', label='Test')\ntraining_legend = mpatches.Patch(color='blue', label='Training')\nplt.legend(handles=[test_legend, training_legend])\nplt.show()\ndisplay()\n"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["# # for presentation\n# temp = pd.DataFrame([])\n\n# for i in np.arange(0, len(complete_lc)):\n  \n#   temp = temp.append(pd.DataFrame({'% of Training Data Used': (i+1)*5, \n#                                    'Training Error': complete_lc[i][0],\n#                                    'Testing Error': complete_lc[i][1]\n#                                   }\n#                                   , index = [i]\n#                                  )\n#                     )\n\n# temp \n\n\n# my_colors = [(1,1,1), (0.93,0.81,0.10), (0.51,0.78,0.65), (0,0.27,0.67)]\n\n# ax = temp.plot.line(x = '% of Training Data Used', y = ['Training Error', 'Testing Error'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# ax.legend(loc='best', fancybox=False, framealpha=0.2)\n\n# ax.set_ylim((0,5))\n# # figure.set_xlim((-1,20))\n\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"markdown","source":["### Evaluation\n\nDespite the fact that we got better results on the reviews only dataset, we perform our evaluations on all the ratings (i.e. ratings in the ratings and reviews datasets). This allows us to make predictions for more users while trading off slightly on accuracy."],"metadata":{}},{"cell_type":"code","source":["(train, test) = ratingsDF_complete.drop(\"Timestamp\").randomSplit([0.7, 0.3], seed)\n\n# Initialize ALS algorithm\nals = (ALS()\n       .setSeed(seed)\n       .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\")\n       .setMaxIter(10)\n       .setRegParam(0.3)\n       .setRank(8)\n      )\n\n# Create the model with these parameters.\nmodel = als.fit(train)\n\n# Run the model to create a prediction. Predict against the validation_df.\npredict_df = model.transform(test)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_ratings_df = (predict_df\n                        .filter(predict_df.prediction != float('nan'))\n                       )\n\npredicted_ratings_df.limit(5).show()"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"markdown","source":["#### Home Page Recommendation\nRecommends the top 20 items that the user might be interested, which the user has not yet reviewed or rated."],"metadata":{}},{"cell_type":"code","source":["# Print Home page prediction for given user\nmy_user_id = 6425496\n\nuser_ratings = ratingsDF_complete.filter(col(\"userId\") == my_user_id)\nprint \"total user ratings\", user_ratings.count()\nall_ratings = ratingsDF_complete.filter(col(\"userId\") != my_user_id)\n(_train, test) = all_ratings.randomSplit([0.8, 0.2], seed)\ntrain = _train.unionAll(user_ratings)\n\nals = (ALS()\n       .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\", nonnegative=True)\n       .setPredictionCol(\"prediction\").setMaxIter(10).setSeed(seed).setRank(8).setRegParam(0.3))\n\nmodel = als.fit(train)\n\npredictions = model.transform(test).filter(col('prediction') != float('nan'))\n\nmy_rated_product_ids = [x[0] for x in user_ratings.select(user_ratings.productId).collect()]\nnot_rated_df = productConversionTableDF.filter(~ productConversionTableDF[\"newProductId\"].isin(my_rated_product_ids))\n\nmy_unrated_products_df = not_rated_df.withColumn('userId', lit(my_user_id)).withColumnRenamed(\"newProductId\", \"productId\")\nnew_predictions = model.transform(my_unrated_products_df).filter(col(\"prediction\") != float('nan'))\n\nproduct_predictions = new_predictions.sort(col(\"prediction\").desc()).limit(20)\n\njoined_df = (product_predictions.withColumnRenamed(\"asin\", \"a\")\n             .join(productConversionTableDF, productConversionTableDF.newProductId == product_predictions.productId)\n             .drop('newProductId')\n             .drop(\"a\"))\n\njoined_meta_df = joined_df.withColumnRenamed(\"asin\", \"a\").join(attributesDF, attributesDF.asin == col(\"a\")).drop(\"a\")\n\nimages = [x[0] for x in joined_meta_df.select('imUrl').filter(col(\"imUrl\").isNotNull()).collect()]\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in images]\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":["Products rated highly by user for comparison"],"metadata":{}},{"cell_type":"code","source":["# user's top rated items\nmy_user_id = 6425496\nreviewerID = userConversionTableDF.filter(col(\"newUserId\") == my_user_id) .collect()[0][0]\n\nusers_top_50 = rawReviewsDF.filter(col(\"reviewerID\") == reviewerID).select(col(\"asin\").alias(\"a\")).filter(\"overall >= 3.0\") # .sort(col(\"overall\").desc())\nusers_top_50_products = users_top_50.join(attributesDF, attributesDF.asin == users_top_50.a).drop('a').select(\"imUrl\")\n\nimages = [x[0] for x in users_top_50_products.filter(col(\"imUrl\").isNotNull()).collect()]\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in images]\n\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"markdown","source":["Baseline - Popularity Recommender"],"metadata":{}},{"cell_type":"code","source":["my_user_id = 6425496\nuser_ratings = ratingsDF_complete.filter(col(\"userId\") == my_user_id)\nmy_rated_product_ids = [x[0] for x in user_ratings.select(user_ratings.productId).collect()]\nnot_rated_df = productConversionTableDF.filter(~ productConversionTableDF[\"newProductId\"].isin(my_rated_product_ids))\n\npopular_items_df = (salesRankDF_newID\n .join(not_rated_df, productConversionTableDF.newProductId == salesRankDF_newID.productId)\n .drop(\"newProductId\")\n .withColumnRenamed(\"asin\", \"a\")\n .withColumnRenamed(\"salesRank\", \"sr\")\n .join(rawMetasDF, rawMetasDF.asin == col(\"a\"))\n .drop(\"a\")\n .drop(\"salesRank\")\n .withColumnRenamed(\"sr\", \"salesRank\")\n .select(col(\"salesRank\").cast(IntegerType()), \"asin\", \"imUrl\", \"title\")).filter(col(\"salesRank\") != float('nan')).sort(col(\"salesRank\")).limit(20)\n\n# print popular_items_df.sort(col(\"salesRank\").desc()).head(15)\n# Popular Books: \nimages = [x[0] for x in popular_items_df.select('imUrl').filter(col(\"imUrl\").isNotNull()).collect()]\nhtml =  [(\"<img style='height:300px;' src ='\" + link + \"'>\") for link in images]\ndisplayHTML(''.join(html))"],"metadata":{},"outputs":[],"execution_count":159},{"cell_type":"markdown","source":["Discussion\n\nUser example above - romance novel why are. Model works but we could improve it better. How about SGD? \nThe examples above shows the performance of the ALS Algorithm when compared to the Popularity recommender as a baseline. The first set of pictures shows the products predicted by the ALS Algorithm that might be of interest to the selected user. The second set of pictures show the products rated highly by this user. And finally, the last set of pictures show the top rated products across all products in Amazon. At a glance, we can see that there is some correlation between the ALS predictions and the products rated highly by the user. There is less correlation between the products rated highly by the user, and the popular products."],"metadata":{}},{"cell_type":"markdown","source":["#### RMSE"],"metadata":{}},{"cell_type":"code","source":["# Create an RMSE evaluator using the label and predicted columns\nreg_eval = (RegressionEvaluator()\n            .setPredictionCol(\"prediction\")\n            .setLabelCol(\"rating\")\n            .setMetricName(\"rmse\")\n           )\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\nerror = reg_eval.evaluate(predicted_ratings_df)\n\nprint(error)"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"markdown","source":["#### Precision - Recall"],"metadata":{}},{"cell_type":"code","source":["# range over which to test\nthresholds = np.arange(0, 5.5, 0.1)\n\n# initialize empty dataframe\nresults = pd.DataFrame([])\n\nfor i in np.arange(0, len(thresholds)):\n  \n  threshold = thresholds[i]\n  \n  temp = (predicted_ratings_df\n          .withColumn('Truth', \n                      when(col('rating')>=4, 1)\n                      .otherwise(0)\n                     )\n          .withColumn('Threshold', lit(threshold))\n          .withColumn('Prediction', \n                      when(col('prediction')>=threshold, 1)\n                      .otherwise(0)\n                     )\n          .withColumn('Class', \n                      when((col('Truth')==1) & (col('Prediction')==1), 'TP')\n                      .when((col('Truth')==0) & (col('Prediction')==1), 'FP')\n                      .when((col('Truth')==0) & (col('Prediction')==0), 'TN')\n                      .when((col('Truth')==1) & (col('Prediction')==0), 'FN')\n                      .otherwise(None)\n                     )\n          .groupBy('Threshold', 'Class')\n          .count()\n         )\n  \n  results = results.append(temp.toPandas())\n  \nresults.head()"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["# create some helper functions\ndef precision(row):\n  # Precision = true-positives / (true-positives + false-positives)\n  result = row['TP'] / (row['TP'] + row['FP']) \n  result = float(result)\n  return result\n\ndef recall(row):\n  # Recall = true-positives / (true-positives + false-negatives)\n  result = row['TP'] / (row['TP'] + row['FN']) \n  result = float(result)\n  return result\n\ndef f_measure(row):\n  # F-measure = 2 x Recall x Precision / (Recall + Precision)\n  result = (2 * row['recall'] * row['precision']) / (row['recall'] + row['precision'])\n  result = float(result)\n  return result"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["temp = (results\n           .pivot(index='Threshold', columns='Class', values='count')\n           .fillna(value = 0)\n           .reset_index()\n          )\n\n# apply functions\ntemp['precision'] = temp.apply(lambda row: precision(row), axis=1)\ntemp['recall'] = temp.apply(lambda row: recall(row), axis=1)\ntemp['f_measure'] = temp.apply(lambda row: f_measure(row), axis=1)\n\n# see results\ntemp.head()"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["display(temp.plot.line(x = 'Threshold', y = ['precision', 'recall', 'f_measure']).figure)"],"metadata":{},"outputs":[],"execution_count":167},{"cell_type":"code","source":["# # for presentation slides\n# my_colors = [(0,0.27,0.67), (0.51,0.78,0.65), (0.93,0.81,0.10)]\n\n# ax = temp.plot.line(x = 'Threshold', y = ['precision', 'recall', 'f_measure'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"markdown","source":["old code below"],"metadata":{}},{"cell_type":"code","source":["# # range over which to test\n# thresholds = np.arange(0, 5.5, 0.1)\n\n# # initialize empty dataframe\n# results = pd.DataFrame([])\n\n# for i in np.arange(0, len(thresholds)):\n  \n#   threshold = thresholds[i]\n  \n#   temp = (predicted_ratings_df\n#           .withColumn('Truth', \n#                       when(col('rating')>=4, 1)\n#                       .otherwise(0)\n#                      )\n#           .withColumn('Reccommendation', \n#                       when(col('prediction')>=threshold, 1)\n#                       .otherwise(0)\n#                      )\n#          )\n  \n#   tp = (temp\n#         .filter(col('Truth')==1)\n#         .filter(col('Reccommendation')==1)\n#         .count()\n#        )\n  \n#   fp = (temp\n#         .filter(col('Truth')==0)\n#         .filter(col('Reccommendation')==1)\n#         .count()\n#        )\n  \n#   tn = (temp\n#         .filter(col('Truth')==0)\n#         .filter(col('Reccommendation')==0)\n#         .count()\n#        )\n  \n#   fn = (temp\n#         .filter(col('Truth')==1)\n#         .filter(col('Reccommendation')==0)\n#         .count()\n#        )\n  \n#   results = results.append(pd.DataFrame({'threshold': threshold, \n#                                          'tp': tp,\n#                                          'fp': fp,\n#                                          'tn': tn,\n#                                          'fn': fn\n#                                         }\n#                                         , index = [i]\n#                                        )\n#                           )\n# results\n\n# # apply functions\n# results['precision'] = results.apply(lambda row: precision(row), axis=1)\n# results['recall'] = results.apply(lambda row: recall(row), axis=1)\n# results['f_measure'] = results.apply(lambda row: f_measure(row), axis=1)\n\n# # see results\n# results"],"metadata":{},"outputs":[],"execution_count":170},{"cell_type":"code","source":["# display(results.plot.line(x = 'threshold', y = ['precision', 'recall', 'f_measure']).figure)"],"metadata":{},"outputs":[],"execution_count":171},{"cell_type":"code","source":["# my_colors = [(0,0.27,0.67), (0.51,0.78,0.65), (0.93,0.81,0.10)]\n\n# ax = results.plot.line(x = 'threshold', y = ['precision', 'recall', 'f_measure'], color = my_colors)\n\n# ax.tick_params(\n#   axis='x',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   bottom='off',      # ticks along the bottom edge are off\n#   top='off',         # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.tick_params(\n#   axis='y',          # changes apply to the x-axis\n#   which='both',      # both major and minor ticks are affected\n#   left='off',        # ticks along the bottom edge are off\n#   right='off',       # ticks along the top edge are off\n#   labelbottom='on')  # labels along the bottom edge are off\n\n# ax.spines['bottom'].set_color('white')\n# ax.spines['top'].set_color('white') \n# ax.spines['right'].set_color('white')\n# ax.spines['left'].set_color('white')\n\n# ax.tick_params(axis='x', colors='white')\n# ax.tick_params(axis='y', colors='white')\n\n# ax.yaxis.label.set_color('white')\n# ax.xaxis.label.set_color('white')\n\n# display(ax.figure)"],"metadata":{},"outputs":[],"execution_count":172},{"cell_type":"markdown","source":["#### Average Execution Times - rank v regParam"],"metadata":{}},{"cell_type":"code","source":["# commented large execution\n\n# # execution times\n# import time\n# # ParamGrid\n# regParams = [0.8, 0.5, 0.3, 0.1, 0.05, 0.01]\n# ranks = [8, 9, 10, 11, 12]\n# ranges = [[0.05, 0.95], [0.1, 0.9], [0.15, 0.85], [0.2, 0.8], [0.25, 0.75], [0.3, 0.7], [0.35, 0.65], [0.4, 0.6], [0.45, 0.55], [0.5, 0.5], [0.55, 0.45], [0.6, 0.4], [0.65, 0.35], [0.7, 0.3], [0.75, 0.25], [0.8, 0.2], [0.85, 0.15], [0.9, 0.1], [0.95, 0.05]]\n\n# results = []\n\n# def predict(df, rank, regParam):\n#   (train, test) = randomSplitTrainTestVal(df, [0.8, 0.2])\n#   errors = []\n#   reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n#   als = (ALS()\n#            .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\")\n#            .setMaxIter(10)\n#            .setSeed(seed)\n#            .setRegParam(regParam)\n#            .setParams(rank=rank))\n#   model = als.fit(train)\n#   predict_df = model.transform(test)\n#   predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n#   test_error = reg_eval.evaluate(predicted_ratings_df)\n#   return test_error\n\n# for regParam in regParams:\n#   for rank in ranks:\n#     durations = []\n#     for i in range(5):\n#       start = time.time()\n#       error = predict(reviewsDF_newID, rank, regParam)\n#       end = time.time()\n#       duration = end-start\n#       durations.append(duration)\n#     avgDuration = reduce(lambda x, y: x + y, durations) / len(durations)\n#     result = dict(error = error, rank= rank, regParam= regParam, avgDuration=avgDuration, durations=durations)\n#     print \"result: \", result\n#     results.append(result)\n    \n# print results"],"metadata":{},"outputs":[],"execution_count":174},{"cell_type":"code","source":["avgDurations = [{'regParam': 0.8, 'durations': [32.629246950149536, 33.00645589828491, 33.302823066711426, 33.27130699157715, 31.821557998657227], 'avgDuration': 32.80627818107605, 'rank': 8, 'error': 1.2203321705754255}, {'regParam': 0.8, 'durations': [34.6021089553833, 34.39513087272644, 33.51827096939087, 34.31026792526245, 34.902360916137695], 'avgDuration': 34.34562792778015, 'rank': 9, 'error': 1.221807579947982}, {'regParam': 0.8, 'durations': [34.56409692764282, 34.54246711730957, 34.77338194847107, 34.296891927719116, 36.073237895965576], 'avgDuration': 34.85001516342163, 'rank': 10, 'error': 1.2205288743565545}, {'regParam': 0.8, 'durations': [36.83195900917053, 36.01732587814331, 37.4742169380188, 35.665250062942505, 36.67460894584656], 'avgDuration': 36.53267216682434, 'rank': 11, 'error': 1.2183948385501917}, {'regParam': 0.8, 'durations': [37.799954891204834, 37.50326204299927, 38.34353280067444, 37.660502910614014, 39.029202938079834], 'avgDuration': 38.06729111671448, 'rank': 12, 'error': 1.2191598280939822}, {'regParam': 0.5, 'durations': [34.12103009223938, 33.55432200431824, 32.75436210632324, 34.23967003822327, 34.39057397842407], 'avgDuration': 33.81199164390564, 'rank': 8, 'error': 1.0615770262069468}, {'regParam': 0.5, 'durations': [35.5602240562439, 34.377705812454224, 33.606611013412476, 34.52962803840637, 32.89467096328735], 'avgDuration': 34.19376797676087, 'rank': 9, 'error': 1.0626657776889408}, {'regParam': 0.5, 'durations': [50.966663122177124, 35.4899218082428, 35.09305000305176, 34.50436210632324, 34.69225597381592], 'avgDuration': 38.14925060272217, 'rank': 10, 'error': 1.0615910235969408}, {'regParam': 0.5, 'durations': [36.90130400657654, 35.52209711074829, 46.03167796134949, 48.22877597808838, 34.691269874572754], 'avgDuration': 40.27502498626709, 'rank': 11, 'error': 1.0599071663952697}, {'regParam': 0.5, 'durations': [37.30120491981506, 37.28495216369629, 39.19371509552002, 36.720272064208984, 36.192981004714966], 'avgDuration': 37.338625049591066, 'rank': 12, 'error': 1.060727002395333}, {'regParam': 0.3, 'durations': [33.8825249671936, 34.29401898384094, 32.0565299987793, 32.917362213134766, 33.20589780807495], 'avgDuration': 33.27126679420471, 'rank': 8, 'error': 1.0126312526258054}, {'regParam': 0.3, 'durations': [34.555712938308716, 33.02296710014343, 33.36643314361572, 34.69293689727783, 36.80426216125488], 'avgDuration': 34.48846244812012, 'rank': 9, 'error': 1.0136808730900195}, {'regParam': 0.3, 'durations': [97.66613578796387, 104.21938014030457, 47.73914408683777, 34.20307993888855, 98.09312295913696], 'avgDuration': 76.38417258262635, 'rank': 10, 'error': 1.0118410472686057}, {'regParam': 0.3, 'durations': [38.85944104194641, 37.06703209877014, 36.569177865982056, 36.079416036605835, 37.96514916419983], 'avgDuration': 37.308043241500854, 'rank': 11, 'error': 1.0094529827509475}, {'regParam': 0.3, 'durations': [35.37493085861206, 35.529764890670776, 36.0109338760376, 37.24548006057739, 36.71477699279785], 'avgDuration': 36.175177335739136, 'rank': 12, 'error': 1.0106059167022554}, {'regParam': 0.1, 'durations': [31.943068981170654, 34.17848610877991, 34.07354187965393, 33.39654994010925, 47.8437020778656], 'avgDuration': 36.28706979751587, 'rank': 8, 'error': 1.146320530504729}, {'regParam': 0.1, 'durations': [33.1464569568634, 33.61918807029724, 32.40067219734192, 33.44556212425232, 33.31796979904175], 'avgDuration': 33.185969829559326, 'rank': 9, 'error': 1.128135892512113}, {'regParam': 0.1, 'durations': [33.00710988044739, 34.71892714500427, 33.539387941360474, 34.04915499687195, 35.79251289367676], 'avgDuration': 34.22141857147217, 'rank': 10, 'error': 1.13294058111402}, {'regParam': 0.1, 'durations': [36.42662215232849, 36.001179933547974, 37.901947021484375, 37.20575499534607, 36.32979607582092], 'avgDuration': 36.77306003570557, 'rank': 11, 'error': 1.1080604641972822}, {'regParam': 0.1, 'durations': [37.348905086517334, 36.93867111206055, 36.17887616157532, 38.64650201797485, 38.60742712020874], 'avgDuration': 37.54407629966736, 'rank': 12, 'error': 1.107211933370861}, {'regParam': 0.05, 'durations': [33.562127113342285, 32.788208961486816, 34.555376052856445, 33.90340304374695, 35.40863108634949], 'avgDuration': 34.0435492515564, 'rank': 8, 'error': 1.5562782194589175}, {'regParam': 0.05, 'durations': [35.81029391288757, 43.64779710769653, 140.89167308807373, 56.54567503929138, 38.90817594528198], 'avgDuration': 63.16072301864624, 'rank': 9, 'error': 1.4549486760825026}, {'regParam': 0.05, 'durations': [40.87470602989197, 180.16276597976685, 104.89376306533813, 51.55047011375427, 55.352210998535156], 'avgDuration': 86.56678323745727, 'rank': 10, 'error': 1.4944022100655403}, {'regParam': 0.05, 'durations': [41.60296988487244, 45.95779204368591, 46.89685392379761, 46.132251024246216, 59.31430697441101], 'avgDuration': 47.980834770202634, 'rank': 11, 'error': 1.390626220667131}, {'regParam': 0.05, 'durations': [52.52171206474304, 41.30989909172058, 70.61736178398132, 67.79461908340454, 77.2807970046997], 'avgDuration': 61.90487780570984, 'rank': 12, 'error': 1.3866168389127873}, {'regParam': 0.01, 'durations': [43.14728879928589, 42.91088390350342, 43.59098196029663, 69.92471694946289, 75.35859203338623], 'avgDuration': 54.98649272918701, 'rank': 8, 'error': 3.653058013032669}, {'regParam': 0.01, 'durations': [37.39614796638489, 48.56079602241516, 41.564960956573486, 43.18789005279541, 47.08396506309509], 'avgDuration': 43.55875201225281, 'rank': 9, 'error': 3.3986463877874393}, {'regParam': 0.01, 'durations': [48.68684792518616, 44.58919405937195, 63.126070976257324, 34.27534794807434, 35.29286599159241], 'avgDuration': 45.19406538009643, 'rank': 10, 'error': 3.447593235500553}, {'regParam': 0.01, 'durations': [36.70055413246155, 36.300585985183716, 35.778634786605835, 138.66716885566711, 37.15303707122803], 'avgDuration': 56.91999616622925, 'rank': 11, 'error': 3.093274271413025}, {'regParam': 0.01, 'durations': [37.25367498397827, 44.96364998817444, 94.21205997467041, 110.95095205307007, 51.0137197971344], 'avgDuration': 67.67881135940551, 'rank': 12, 'error': 3.1342055148356445}]\n\navgDurationsDf = sc.parallelize(avgDurations).toDF()\navgDurationsSortedDf = (avgDurationsDf\n                        .sort(avgDurationsDf.avgDuration)\n                        .withColumn(\"avg(s)\", round(col(\"avgDuration\"), 2))\n                        .withColumn(\"rmse\", round(col(\"error\"), 3))\n                        .select(\"avg(s)\", \"rmse\", \"rank\", \"regParam\"))\navgDurationsSortedDf.show()"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["display(avgDurationsSortedDf)"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["display(avgDurationsSortedDf)"],"metadata":{},"outputs":[],"execution_count":177},{"cell_type":"markdown","source":["The above two graphs show how the amount of time taken to train our model using different combinations hyper parameters. There is a clear trade-off between model training time and accuracy. Based on our best \"business\" judgement we chose to set our regularization parameter to 0.3 and rank as 8."],"metadata":{}},{"cell_type":"markdown","source":["#### Conclusions\n\nWe were able to show both anecdotally and mathematically the impressive performance of our ALS model."],"metadata":{}},{"cell_type":"markdown","source":["## Collaborative Filtering with Stratified Stochastic Gradient Descent  \n\nFor a fully online or real-time model, we explored SSGD as an alternative method for deriving U and V for our matrix factorization model. In his [paper](http://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf), Christopher R. Aberger show that stochastic gradient descent is generally faster and more accurate than ALS except in situations of sparse data in which ALS tends to performs better. Motivated by this research, we decided to try our hand at this model. The following is a brief summary of the method.     \n\n#### SSGD\n\nThe Stratified Stochastic Gradient Descent is a particular implementation of the Stochastic Gradient Descent made for distributed environments and it is particularly suitable for map/reduce implementations. The stratification consists on dividing the ratings matrix and the two matrices H and W produced by the factorization algorithm into blocks. The blocks will form a new matrix and the block is the unit of computation, meaning that the number of blocks should be higher than the number of cores in your cluster. At the same time, the algorithm will perform better with fewer blocks, which means we need to find the smallest value of blocks that optimizes the computation between error and speed. My implementation uses a parallelization rank. I calculated the rank doing the ceiling of the square root of the number of cores of the cluster. Each stratus is composed by diagonal blocks to make sure that the same block of H and W is not present in two blocks inside the same stratus. A stratus has access to a complete H and a complete W during every iteration. At the stratus level, we generally compute only the regularization term which requires access to the full H and W. However, this implementation doesn't include the regularization parameter because I didn't have enough time to get to the end of the implementation. Each stratus is merged at the end through a weighted sum of the H and W calculated. However, most of our dataset contains few ratings per user and they should be well distributed in all blocks because of the way how we preprocessed our data. We decided to skip the implementation of weighting and we used a simple average, filtering the entries of H and W in the blocks that do not contain any rating for the user/product.\n\nThis is just a proof of concept. A production implementation would require all those aspects to be covered. We don't expect good results from this model given the variability of the data. However, it is still interesting to see how it looks like an algorithm that could potentially learn online.\n\nDue to the lack of any existing implementation of SGD in Spark, we had to write the full implementation by hand as follows."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom pyspark.sql.functions import col\nfrom pyspark import StorageLevel\n\n\nclass SSGD:\n    \"\"\"\n    This is the implementation of the Stratified Stochastic Gradient Descent presented by IBM in the article\n    \"Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent\". This is not meant to be\n    a final implementation ready for the production environment, but just a proof of concept. Ultimately,\n    this algorithm uses RDDs which means it would be faster if reimplemented in scala.\n\n    \"\"\"\n    def __init__(self, maxIter=10, parallelizationRank=12, nFactors=8, sigma=0.1, lambda_input=0.3):\n        self.iter = maxIter\n        self.nExecutors = parallelizationRank\n        self.factors = nFactors\n        self.sigma = sigma\n        self.lambda_input = lambda_input\n        self.total_update_count = -1\n\n\n    def SGD_update(self, t):\n        # get all three items\n        V_block, W_block, H_block = t\n\n        # for each item in each tuple\n        for (product_id, user_id, rating) in V_block:\n\n            Wi = W_block[product_id].copy()\n            Hj = H_block[user_id].copy()\n            \n            W_block[product_id] = ((1-self.sigma)*W_block[product_id])+(self.sigma*(np.dot(np.dot(1.0/np.dot(Hj, Hj.T), Hj), rating)))\n            H_block[user_id] = ((1-self.sigma)*H_block[user_id])+(self.sigma*(np.dot(np.dot(1.0/np.dot(W_block[product_id].T,W_block[product_id]),W_block[product_id].T), rating)))\n            \n        return (V_block, W_block, H_block)\n      \n    def filterWH(self, entry):\n        \"\"\"\n          This function filters out the entries of W and H that do not contain any rating in the block\n        \"\"\"\n        (V_block, W_block, H_block) = entry\n        V_w = np.array(reduce(lambda a,b: a+b, map(lambda (pid, uid, r): [pid], V_block)))\n        V_h = np.array(reduce(lambda a,b: a+b, map(lambda (pid, uid, r): [uid], V_block)))\n        W_filtered = dict(filter(lambda (k,v): k in V_w, W_block.iteritems()))\n        H_filtered = dict(filter(lambda (k,v): k in V_h, H_block.iteritems()))\n        return (V_block, W_filtered, H_filtered)\n\n    def trainSSGD(self, VDF, userCol=\"userId\", productCol=\"productId\", ratingCol=\"rating\"):\n\n        # data in a dataframe\n        ratings_data = VDF.select(col(productCol), col(userCol), col(ratingCol)) #.persist(StorageLevel.MEMORY_AND_DISK)\n\n        num_products = ratings_data.select(productCol).distinct().count()\n        num_users = ratings_data.select(userCol).distinct().count()\n\n        # global varibale to keep track of all previous itertaions\n        self.total_update_count = 0\n\n        # initilized W and H with same number of values as of number of users and movies\n        # randomizing according to factors provided by the user\n        W = ratings_data.select(col(productCol)).distinct().rdd.map(lambda x: (x[0], np.random.rand(self.factors)-0.5))\n        H = ratings_data.select(col(userCol)).distinct().rdd.map(lambda x: (x[0], np.random.rand(self.factors)-0.5))\n\n        #the initial values of W and H divided in blocks\n        Wib = W.map(lambda x: (x[0] % self.nExecutors, [x])).reduceByKey(lambda a, b: a + b)\n        Hib = H.map(lambda x: (x[0] % self.nExecutors, [x])).reduceByKey(lambda a, b: a + b)\n\n        #we get the indexes of the partitions created (some could potentially be empty, those will be excluded)\n        userBlocks = Hib.reduceByKey(lambda a, b: a).map(lambda x: x[0])\n        productBlocks = Wib.reduceByKey(lambda a, b: a).map(lambda x: x[0])\n        \n        #getting the same block replicated by the number of the partitions of the other matrix (I will need this later)\n        W = Wib.cartesian(userBlocks).map(lambda x: ((x[0][0], x[1]), x[0][1]))\n        H = Hib.cartesian(productBlocks).map(lambda x: ((x[1], x[0][0]), x[0][1]))\n\n        # get blocks of data matrices divided by index.\n        V_blocks = ratings_data.rdd.map(lambda x: ((x[0] % self.nExecutors, x[1] % self.nExecutors), [(x[0], x[1], x[2])]))\n\n        #assemble the partitioned dataset getting one block per row in the RDD\n        V_group = V_blocks.reduceByKey(lambda a, b: a + b).leftOuterJoin(W).leftOuterJoin(H).map(lambda x: (x[0], (x[1][0][0], dict(x[1][0][1]), dict(x[1][1]))))\n        \n        #drop H and W entries that don't have at least one rating in the block\n        VWH_group = V_group.mapValues(self.filterWH).cache()\n\n        iterations = 0\n\n        # to keep track of total number of SGD updates made across all strata\n        curr_upd_count = V_blocks.count()\n\n        # run till number of iterations provided by user\n        while iterations < self.iter:\n            print \"Iterations: %d/%d\" % (iterations+1, self.iter)\n\n            # group Vblock, Wib and Hib to send it to SGD update\n            VWH_group = VWH_group.mapValues(self.SGD_update)\n\n            # update total updates or 'n' in algorithm 2 after each iteration\n            self.total_update_count += curr_upd_count\n\n            # increment the loop\n            iterations += 1\n\n        W_merged = VWH_group.flatMap(lambda x: x[1][1].items()).cache() \n        Ni = W_merged.countByKey()\n        \n        H_merged = VWH_group.flatMap(lambda x: x[1][2].items()).cache() \n        Nj = H_merged.countByKey()\n        \n        W = W_merged.reduceByKey(lambda a, b: a + b).map(lambda v: (v[0], v[1] / float(Ni[v[0]])))\n        H = H_merged.reduceByKey(lambda a, b: a + b).map(lambda v: (v[0], v[1] / float(Nj[v[0]])))\n        return (W, H)\n"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"code","source":["(split_70_df, split_b_30_df) = ratingsDF_complete.randomSplit([0.7, 0.3], seed)\n\ntraining_df = split_70_df.cache()\ntest_df = split_b_30_df.cache()\nssgd = SSGD()\n\n(productFeatures,userFeatures) = ssgd.trainSSGD(training_df)"],"metadata":{},"outputs":[],"execution_count":182},{"cell_type":"code","source":["#loading model previously generated and saved in parquet files\n(split_70_df, split_b_30_df) = ratingsDF_complete.randomSplit([0.7, 0.3], seed)\n\ntraining_df = split_70_df.cache()\ntest_df = split_b_30_df.cache()\n\nfrom pyspark.mllib.linalg import Vectors\nproductFeaturesDF = sqlContext.read.parquet(\"/mnt/%s/parquetDataset/results-productFeatures-full.parquet\" % MOUNT_NAME).coalesce(128)\nuserFeaturesDF = sqlContext.read.parquet(\"/mnt/%s/parquetDataset/results-userFeatures-full.parquet\" % MOUNT_NAME).coalesce(128)\nproductFeatures = productFeaturesDF.rdd.map(lambda x: (x[0], Vectors.dense([x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8]])))\nuserFeatures = userFeaturesDF.rdd.map(lambda x: (x[0], Vectors.dense([x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8]])))\n"],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["print productFeatures.take(1)\nprint userFeatures.take(1)\nuserFeaturesDF = sqlContext.createDataFrame(userFeatures)\ndisplay(userFeaturesDF)"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"code","source":["def predict(entry):\n  \"\"\"\n  Inputs: \n    \n    user (integer): userId\n    \n    item (integer): itemId\n    \n    userFactors (np.array): Features array\n                              \n    itemFactors (np.array): Features array\n    \n  Outputs: \n    \n    user_item_rating (float): Predicted rating for item by user\n    \n    \n  \"\"\"\n  (item, user, itemFactors, userFactors) = entry\n  # extract features for user\n  user_features = np.asarray(userFactors, dtype=np.float64\n                            )\n  \n  # extract features for item\n  item_features = np.asarray(itemFactors, dtype=np.float64\n                            ) \n  if (np.count_nonzero(~np.isnan(user_features)) == 0 or np.count_nonzero(~np.isnan(item_features)) == 0):\n    #in case I have no data to make the prediction I will choose an equidistant number to minimize the error\n    return (user, item, float(3))\n  \n  # dot product of user_features and item_features\n  user_item_rating = user_features.dot(item_features)\n  \n  return (user, item, float(user_item_rating))"],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"code","source":["#generate predictions and prepare dataset for evaluation\nuserFeaturesDF = sqlContext.createDataFrame(userFeatures).select(col(\"_1\").alias(\"userId\"), col(\"_2\").alias(\"userFeatures\"))\nproductFeaturesDF = sqlContext.createDataFrame(productFeatures).select(col(\"_1\").alias(\"productId\"), col(\"_2\").alias(\"productFeatures\"))\ntest_prediction_df = test_df.join(userFeaturesDF, test_df.userId == userFeaturesDF.userId, \"left\").join(productFeaturesDF, test_df.productId == productFeaturesDF.productId, \"left\").select(test_df.productId,test_df.userId,col(\"productFeatures\"),col(\"userFeatures\")).rdd.map(lambda x: (x[0], x[1], x[2], x[3])).map(predict).toDF().select(col(\"_1\").alias(\"productId\"), col(\"_2\").alias(\"userId\"),col(\"_3\").alias(\"prediction\"))\n\ntest_complete_df = test_prediction_df.join(test_df, (test_prediction_df.userId == test_df.userId) & (test_prediction_df.productId == test_df.productId)).select(test_df.userId, test_df.productId, test_df.rating, test_prediction_df.prediction)"],"metadata":{},"outputs":[],"execution_count":186},{"cell_type":"code","source":["display(test_prediction_df)"],"metadata":{},"outputs":[],"execution_count":187},{"cell_type":"code","source":["#rmse evaluation\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\nerror = reg_eval.evaluate(test_complete_df)\nprint error"],"metadata":{},"outputs":[],"execution_count":188},{"cell_type":"markdown","source":["###Comparison with ALS"],"metadata":{}},{"cell_type":"code","source":["# Initialize ALS algorithm\nals = (ALS()\n       #.setSeed(seed)\n       .setParams(userCol=\"userId\", itemCol=\"productId\", ratingCol=\"rating\")\n       .setMaxIter(5)\n       .setRegParam(0.03)\n       .setRank(8)\n      )\n\n# Create the model with these parameters.\nmodel = als.fit(training_df)\nALSpredictionsDF = model.transform(test_df)\n"],"metadata":{},"outputs":[],"execution_count":190},{"cell_type":"markdown","source":["## Final Conclusions \n\nWhile recommending products using the Book ratings and reviews datasets, we found that providing recommendations based on the reviews dataset alone yields a lower RMSE compared to the ratings dataset alone, or the rating+reviews dataset. This is likely due to written reviews leading to a more deliberate consideration of the product's rating. However, we chose to combine the two datasets to maximise the data points available for the algorithm since the two datasets are already sparse. \n\nCollaborative Filtering and Content Based Recommendations are the two pieces of our recommendation system. Collaborative filtering is a great tool for discovering new products that the user would otherwise be unaware of. This is because it considers the product ratings provided by the users in conjunction with the product ratings provided by other similar users of the platform.  \n\nThe Content Based Recommendation system, unlike the collaborative model, which considers users' ratings for products, Content Based filtering starts at the products and recommends similar products based on the description and common features of the products. So while the Collaborative recommendation works by taking the user's ratings and products as a whole, content based recommendation works by starting from the product selected by the user. In other words, Content Based Filtering is the second piece that completes our recommendation system. \n\n![Amazon_home_page_mock](https://lh3.googleusercontent.com/mPRNHoFv0Fn8TpvSMO2tig-4y1hLqCbjzF_BR-AP1XnClt5it43aX38JhxrnI12x32wOzluW_r2kR8c=w1366-h659\n \"image description\")"],"metadata":{}},{"cell_type":"markdown","source":["## Further areas of improvement \n\n- Rewrite in scala to improve performances\n- Improve the implementation of algorithms \n- Implement Spark Streaming for online prediction and training\n- Add autotests to make it more robust\n- Tune up the models\n- Semantic understanding for Content Based\n- ngrams for Content Based\n- Graph Network model for related products\n\n#### Future areas for research\n\nDue to time and resource constraints, we were unable to explore all possible areas of research. Some of our ideas are documented here and we encourage others to take these forward. \n\n#### Graph Network model\n\nWe were provided data on how different products are bought/viewed together. This information naturally lends itself to graph network based approaches to building a basic reccommendations system. A natural advantage of this approach is better reccommendations for complementary products.      \n\n#### Visual features model\n\nWe were also provided with data pertaining to the visual features of each product. We believe it is possible to use these as features for finding similar products. Our hypothesis is that despite the old adagae, consumers will often judge a book by its cover and that therefore there may be predictive information to be mined from visual features for making reccommendations.       \n\n#### User Bias\n\nMost recommender systems perform better if user and item biases are taken into account. Suppose we have a ratings system that allows each user to rate each item on a scale of 1 to 5 stars. Suppose we have two users: Alice, who rates items with an average of 4 stars, and Bob, whose average rating is 1.5 stars. If Bob rates some new item with 3 stars, it means something very different than if Alice rates the same item with 3 stars (Bob really liked the new item, Alice didn't). The difference is what we call user bias. Modelling for this bias can naturally improve recommendations.  \n\n#### Ensemble model\n\nIts likely that each of these models would do well in certain areas and worse in others. Since most of the models suggested are sufficiently uncorrelated (as they all use different data), an ensemble approach would be a valid modelling technique and likely would achieve better results."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":193}],"metadata":{"name":"Diego","notebookId":31},"nbformat":4,"nbformat_minor":0}
